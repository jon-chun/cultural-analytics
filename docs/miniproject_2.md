# Week: Mini-Project #2
---

# **UPDATE**

[![AI Generated Images from Lyrics to 'Don't Stop Me Now' by Queen](images/cover_dont_stop_me_now_queen.jpg)](https://www.youtube.com/watch?v=l6Myv8yF9zw&list=RDnyD6g47DHQk&index=4)Click on image above to watch Midjourney AI-generated video using lyrics to "Don't Stop Me Now" by Queen

## Overview

This week we bring together all the many disparate and not-so-disparate skills honed over the past several weeks into Mini-Project #2. Over the past few months, text to image (and now video) generation has seen a dramatic leap forward with the introduction of new large AI models. This also aligns nicely with our own AI research, so we can lend an unusual degree of experience to this rapdily evolving field.

Creating text prompts (prompt engineering) to feed into these text2image models is one of the hottest areas of AI research recently, and we'll explore this further this week. This involves an all-hands-on-deck class project that decomposes nicely into smaller groups: text to image generation by recently released state-of-the-art large DNN models.

**GOAL:**

Research, search and scrape Twitter for images and prompts based upon the following 3 state-of-the-art text2image deep neural network (DNN) models:

1. [DALL-E 2](https://openai.com/dall-e-2/): [@openai](https://twitter.com/OpenAI)
2. Midjourney: [@midjourney](https://twitter.com/midjourney?ref_src=twsrc%5Egoogle%7Ctwcamp%5Eserp%7Ctwgr%5Eauthor)
3. [Stable Diffusion](https://stablediffusionweb.com/): [@stablediffusion](https://mobile.twitter.com/stablediffusion)

Our goal is to use NLP on these tweets to extract out the author/organization and text prompt long with the associated generated text2image creation. 

In class will be assigne everyone into three groups, one for each of the main text2image DNN models listed above. Each group will independently research, scrape and analyze as much data as they can for their assigned DNN model. Then as a group, we will combine, compare, and critique our findings as a unified team.


## Readings


- [Monday]:
    * [Lexica.art](https://lexica.art/) Scroll through, search and roll-over to view text prompt that generated each image
    * [The absolute beginners guide to MidJourney AI. Starting with AI Art](https://www.youtube.com/watch?v=PqCIUniQ_U8) (start at 20:00 and peruse for a 5-10 minutes to get a sense of how to interactively design prompt for image generation)
    * [Design Guidelines for Prompt Engineering Text-to-Image Generative Models (7:33)](https://www.youtube.com/watch?v=7-XnIuH8r3U) Brief overview of [ACM paper](https://arxiv.org/pdf/2109.06977.pdf) on prompt engineering experiments 
    * Research all 3 models including (start with the links at the top of this page under GOALS)
        * Websites (official and tutorials)
        * Reddit subthreads (r/subreddits)
        * Twitter (official and tutorials)
        * Twitter (artists and prgorammers)
    * [DALL-e Prompt Book](https://dallery.gallery/the-dalle-2-prompt-book/)


- [Wednesday]:
    * Coming...

- [Friday]:
    * Coming...

## References

* [Promptbase: A marketplace for text engineering](https://promptbase.com/)
* [Awesome Prompt Papers](https://github.com/thunlp/PromptPapers)
