{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome IPHS 290: Computational Cultural Analytics Prof Elkins & Chun Integrated Program for Humane Studies Kenyon College Fall 2022 Overview Computational cultural analytics refers to the collection, analysis, modeling and visualization of data for the exploration of contemporary and historical cultures. Driven by domain expertise from study in traditional fields like political science or literary criticism, computational methods augment research by incorporating the analysis of data created by constituent entities and their interactions (e.g. social media, government documents, contextualizing financial narratives, etc). This is an advanced methods course focused on learning to work with complex data types. The topics learned in this course will dramatically expand the possibilities of your future research in cultural analysis and generalize to analysis in any domain. This course presumes some coding experience or the introductory course to Digital Humanities, IPHS 200 Programming Humanity . The most popular stereotype of data is in the form of relatively small and structured numeric data commonly found in spreadsheets. In fact, the the most common and fastest growing type of data is unstructured data: raw text, sound, images and videos. We compile, transform, model, visualize and analyze both of these datatype in IPHS 300: AI for the Humanities . This course introduces methods to work with different and popular composite datatypes including geospatial, time series and network graphs. These big 3 composite data types each have distinct internal structure based upon spatial, temporal or semantic correlations. Each of these data types have their own distinctive methods, libraries and critiques which we explore via both leading academic DH scholarship and practical technical implementations. Here is an outline of the course: We begin the course with visualizing and analyzing geospatial data via maps and charts. We\u2019ll build our skills with web scraping and API\u2019s to create original datasets from websites and services like Twitter, Reddit and Instagram. Then we\u2019ll study a variety of natural language processing (NLP) subjects from data wrangling to vectorization and topic modeling to state-of-the-art (SOTA) deep neural nets. Narrative sentiment analysis and SentimentArcs provide a NLP framework to explore the particularities of time-series analysis . Graph theory and network analysis introduce new methods to visualize, connect and compute metrics of related entities based on network representations. We study hands-on projects like analyzing the social network of Game of Thrones and trying to classify who\u2019s tweeting: Trump or Trudeau. The final week will focus on a variety of options to share presentations, ML/AI models or applications on the web. This includes several free and paid web hosting options from no-code static blog platforms to dynamic web full-stack virtual servers on cloud hosts like Amazon Web Services. Students produce a wide variety of individualized stand-alone projects to demonstrate specific learned skills. The course culminates in a final class project based on each student's intellectual interests and personal passions. These compelling narratives are reconceived and implemented using any combination of the technologies and techniques presented throughout the semester.","title":"Home"},{"location":"#welcome","text":"IPHS 290: Computational Cultural Analytics Prof Elkins & Chun Integrated Program for Humane Studies Kenyon College Fall 2022","title":"Welcome"},{"location":"#overview","text":"Computational cultural analytics refers to the collection, analysis, modeling and visualization of data for the exploration of contemporary and historical cultures. Driven by domain expertise from study in traditional fields like political science or literary criticism, computational methods augment research by incorporating the analysis of data created by constituent entities and their interactions (e.g. social media, government documents, contextualizing financial narratives, etc). This is an advanced methods course focused on learning to work with complex data types. The topics learned in this course will dramatically expand the possibilities of your future research in cultural analysis and generalize to analysis in any domain. This course presumes some coding experience or the introductory course to Digital Humanities, IPHS 200 Programming Humanity . The most popular stereotype of data is in the form of relatively small and structured numeric data commonly found in spreadsheets. In fact, the the most common and fastest growing type of data is unstructured data: raw text, sound, images and videos. We compile, transform, model, visualize and analyze both of these datatype in IPHS 300: AI for the Humanities . This course introduces methods to work with different and popular composite datatypes including geospatial, time series and network graphs. These big 3 composite data types each have distinct internal structure based upon spatial, temporal or semantic correlations. Each of these data types have their own distinctive methods, libraries and critiques which we explore via both leading academic DH scholarship and practical technical implementations. Here is an outline of the course: We begin the course with visualizing and analyzing geospatial data via maps and charts. We\u2019ll build our skills with web scraping and API\u2019s to create original datasets from websites and services like Twitter, Reddit and Instagram. Then we\u2019ll study a variety of natural language processing (NLP) subjects from data wrangling to vectorization and topic modeling to state-of-the-art (SOTA) deep neural nets. Narrative sentiment analysis and SentimentArcs provide a NLP framework to explore the particularities of time-series analysis . Graph theory and network analysis introduce new methods to visualize, connect and compute metrics of related entities based on network representations. We study hands-on projects like analyzing the social network of Game of Thrones and trying to classify who\u2019s tweeting: Trump or Trudeau. The final week will focus on a variety of options to share presentations, ML/AI models or applications on the web. This includes several free and paid web hosting options from no-code static blog platforms to dynamic web full-stack virtual servers on cloud hosts like Amazon Web Services. Students produce a wide variety of individualized stand-alone projects to demonstrate specific learned skills. The course culminates in a final class project based on each student's intellectual interests and personal passions. These compelling narratives are reconceived and implemented using any combination of the technologies and techniques presented throughout the semester.","title":"Overview"},{"location":"about/","text":"About Katherine Elkins, Ph.D Email: elkinsk@kenyon.edu Google Scholar Katherine Elkins has written over a dozen articles on memory, consciousness, and embodied aesthetic experience in a wide range of writers from Plato and Sappho to Wordsworth and Woolf. In Proust\u2019s \u201cIn Search of Lost Time: Philosophical Perspectives (OUP, 2022), she reframed Proust\u2019s exploration of consciousness in light of integrated information theory. In The Shapes of Stories (Cambridge UP, 2022), she used the AI software SentimentArcs to develop the first robust methodology for exploring the emotional arcs of stories. Her audible.com lectures on \u201cThe Giants of French Literature\u201d and \u201cThe Modern Novel\u201d have won her an international audience. With a grant from the NEH, she and AI researcher Jon Chun created immensely popular computational courses for humanists, social scientists, and artists in the world\u2019s first human-centered AI curriculum. These courses bring a more diverse set of voices to today\u2019s big questions by building a foundation in the conceptual, ethical, and technical aspects of AI from a multi-disciplinary perspective. She is also Co-Founder and Co-Director of the KDH lab focused on the latest advances in natural language processing and generation. Over the past six years, she and Chun have been involved in hundreds of innovative projects applying AI to literature, art, political science, economics, and more. Part of their research has included training GPT language models to write in the style of a variety of authors and styles. In honor of \u010capek\u2019s 100th anniversary of \u201cRobots,\u201d they debuted AI Divabot in collaboration with Director and Artist-in-Residence at the Wexner Center, Jim Dennen. AREAS OF EXPERTISE Professor Elkins\u2019 research interests include literature and philosophy, modernist studies, cognitive studies, artificial intelligence, digital humanities and cultural analytics. EDUCATION 2002 \u2014 Doctor of Philosophy from Univ. of California Berkeley 1990 \u2014 Bachelor of Arts from Yale University RECENT COURSES CWL/IPHS 191/200 \u2013 Programming Humanity (2017-2022) IPHS 290 - Cultural Analytics (2022) CWL/IPHS 391/300 \u2013 Artificial Intelligence for the Humanities (2018-2022) IPHS Senior Seminar (Computational DH) - (2015-2022) Jon Chun Email: chunj@kenyon.edu Twitter: @jonchun2000 Google Scholar Jon Chun has degrees in electrical engineering, computer science and biomedical engineering with a focus on cognitive science from UC Berkeley and UT Austin. As a postgraduate American Heart Research Fellow, he conducted and published research in gene therapy and medical informatics. He has also worked for the Advanced Light Source Group at Lawrence Berkeley Laboratory and the semiconductor research consortium SEMATECH in Austin. His research interests include Machine Learning, Artificial Intelligence, Data Analytics, Natural Language Processing, Affective AI and Explainable AI. Professor Chun has co-founded several startups including as CEO and COO of the world\u2019s largest anonymity service backed by large Wall Street hedge funds and the CIA\u2019s venture fund In-Q-Tel. His specialization in cross-cultural experiences of technology stems from working and studying throughout the US, Europe, Asia and Latin America including the US Foreign Service Spanish Exam and the \u65e5\u672c\u8a9e\u80fd\u529b\u8a66\u9a13 (Japanese). Before arriving in Gambier he was a Director of Development for the world\u2019s largest computer security corporation based in Silicon Valley and Entrepreneur in Residence at UC Berkeley. Jon is interested in bringing diverse voices to urgent debates surrounding technology\u2019s growing impact on society. In 2017 he co-created the world's first AI curriculum in Computational Digital Humanities to bridge the gap between technology and humanity. He publishes on AI and has mentored hundreds of interdisciplinary projects that synthesize Artificial Intelligence with literature, history, political science, art, dance, music, law, medicine, economics and other subjects. AREAS OF EXPERTISE Data Analytics, Machine Learning and AI, NLP, Time Series, Network Security, FinTech, MedTech, XAI/FATE, Chatbots, IoT. EDUCATION 1996 - American Heart Assoc Research Fellow, University of Iowa Medical School 1995 - Master of Science from University of Texas at Austin 1993 - MIT Japan Program Scholar (sponsored NSF and US Navy) 1989 - Bachelor of Science from Univ. of California Berkeley RECENT COURSES CWL/IPHS 191/200 \u2013 Programming Humanity (2017 \u2013 2022) IPHS 290 - Cultural Analytics (2022) CWL/IPHS 391/300 \u2013 Artificial Intelligence for the Humanities (2018 \u2013 2021) IPHS Senior Seminar (Computational DH) - (2020-2022) SciComp Independent Study: Industrial Internet of Things (IIoT) Strategic and Technical Consultating Project IPHS Computational Digital Humanities Research See previous Research and Student Projects in IPHS at Digital Kenyon - Digital Humanities Digital Humanities and DHColab Digital humanities at Kenyon encompasses a dual lens. We empower the next generation of thinkers with the conceptual framework underlying our Age of Information \u2014 from dataism and algorithmic thinking to synthetic biology and artificial intelligence. Our unique approach to computational thinking positions students to engage with the many practical, theoretical and ethical issues surrounding technological innovation and social change. Digital humanities also explores the most recent computational approaches as a way to augment \u2014 rather than replace \u2014 more traditional humanist inquiry. Students imagine and pursue new avenues of research by identifying unexplored datasets of text, image and sound, while embracing new computational frameworks that are increasingly powerful and easy to use. We focus on finding interdisciplinary solutions to today's challenges. Students interested in digital humanities are encouraged to start with our introductory course, \"Programming Humanity.\" Advanced courses include \"A.I. for the Humanities.\" All digital humanities courses are project-based, and students will have a portfolio of innovative projects by the end of their course of study. Learn more at Kenyon Digital Humanities Website","title":"About"},{"location":"about/#about","text":"","title":"About"},{"location":"about/#katherine-elkins-phd","text":"Email: elkinsk@kenyon.edu Google Scholar Katherine Elkins has written over a dozen articles on memory, consciousness, and embodied aesthetic experience in a wide range of writers from Plato and Sappho to Wordsworth and Woolf. In Proust\u2019s \u201cIn Search of Lost Time: Philosophical Perspectives (OUP, 2022), she reframed Proust\u2019s exploration of consciousness in light of integrated information theory. In The Shapes of Stories (Cambridge UP, 2022), she used the AI software SentimentArcs to develop the first robust methodology for exploring the emotional arcs of stories. Her audible.com lectures on \u201cThe Giants of French Literature\u201d and \u201cThe Modern Novel\u201d have won her an international audience. With a grant from the NEH, she and AI researcher Jon Chun created immensely popular computational courses for humanists, social scientists, and artists in the world\u2019s first human-centered AI curriculum. These courses bring a more diverse set of voices to today\u2019s big questions by building a foundation in the conceptual, ethical, and technical aspects of AI from a multi-disciplinary perspective. She is also Co-Founder and Co-Director of the KDH lab focused on the latest advances in natural language processing and generation. Over the past six years, she and Chun have been involved in hundreds of innovative projects applying AI to literature, art, political science, economics, and more. Part of their research has included training GPT language models to write in the style of a variety of authors and styles. In honor of \u010capek\u2019s 100th anniversary of \u201cRobots,\u201d they debuted AI Divabot in collaboration with Director and Artist-in-Residence at the Wexner Center, Jim Dennen. AREAS OF EXPERTISE Professor Elkins\u2019 research interests include literature and philosophy, modernist studies, cognitive studies, artificial intelligence, digital humanities and cultural analytics. EDUCATION 2002 \u2014 Doctor of Philosophy from Univ. of California Berkeley 1990 \u2014 Bachelor of Arts from Yale University RECENT COURSES CWL/IPHS 191/200 \u2013 Programming Humanity (2017-2022) IPHS 290 - Cultural Analytics (2022) CWL/IPHS 391/300 \u2013 Artificial Intelligence for the Humanities (2018-2022) IPHS Senior Seminar (Computational DH) - (2015-2022)","title":"Katherine Elkins, Ph.D"},{"location":"about/#jon-chun","text":"Email: chunj@kenyon.edu Twitter: @jonchun2000 Google Scholar Jon Chun has degrees in electrical engineering, computer science and biomedical engineering with a focus on cognitive science from UC Berkeley and UT Austin. As a postgraduate American Heart Research Fellow, he conducted and published research in gene therapy and medical informatics. He has also worked for the Advanced Light Source Group at Lawrence Berkeley Laboratory and the semiconductor research consortium SEMATECH in Austin. His research interests include Machine Learning, Artificial Intelligence, Data Analytics, Natural Language Processing, Affective AI and Explainable AI. Professor Chun has co-founded several startups including as CEO and COO of the world\u2019s largest anonymity service backed by large Wall Street hedge funds and the CIA\u2019s venture fund In-Q-Tel. His specialization in cross-cultural experiences of technology stems from working and studying throughout the US, Europe, Asia and Latin America including the US Foreign Service Spanish Exam and the \u65e5\u672c\u8a9e\u80fd\u529b\u8a66\u9a13 (Japanese). Before arriving in Gambier he was a Director of Development for the world\u2019s largest computer security corporation based in Silicon Valley and Entrepreneur in Residence at UC Berkeley. Jon is interested in bringing diverse voices to urgent debates surrounding technology\u2019s growing impact on society. In 2017 he co-created the world's first AI curriculum in Computational Digital Humanities to bridge the gap between technology and humanity. He publishes on AI and has mentored hundreds of interdisciplinary projects that synthesize Artificial Intelligence with literature, history, political science, art, dance, music, law, medicine, economics and other subjects. AREAS OF EXPERTISE Data Analytics, Machine Learning and AI, NLP, Time Series, Network Security, FinTech, MedTech, XAI/FATE, Chatbots, IoT. EDUCATION 1996 - American Heart Assoc Research Fellow, University of Iowa Medical School 1995 - Master of Science from University of Texas at Austin 1993 - MIT Japan Program Scholar (sponsored NSF and US Navy) 1989 - Bachelor of Science from Univ. of California Berkeley RECENT COURSES CWL/IPHS 191/200 \u2013 Programming Humanity (2017 \u2013 2022) IPHS 290 - Cultural Analytics (2022) CWL/IPHS 391/300 \u2013 Artificial Intelligence for the Humanities (2018 \u2013 2021) IPHS Senior Seminar (Computational DH) - (2020-2022) SciComp Independent Study: Industrial Internet of Things (IIoT) Strategic and Technical Consultating Project","title":"Jon Chun"},{"location":"about/#iphs-computational-digital-humanities-research","text":"See previous Research and Student Projects in IPHS at Digital Kenyon - Digital Humanities","title":"IPHS Computational Digital Humanities Research"},{"location":"about/#digital-humanities-and-dhcolab","text":"Digital humanities at Kenyon encompasses a dual lens. We empower the next generation of thinkers with the conceptual framework underlying our Age of Information \u2014 from dataism and algorithmic thinking to synthetic biology and artificial intelligence. Our unique approach to computational thinking positions students to engage with the many practical, theoretical and ethical issues surrounding technological innovation and social change. Digital humanities also explores the most recent computational approaches as a way to augment \u2014 rather than replace \u2014 more traditional humanist inquiry. Students imagine and pursue new avenues of research by identifying unexplored datasets of text, image and sound, while embracing new computational frameworks that are increasingly powerful and easy to use. We focus on finding interdisciplinary solutions to today's challenges. Students interested in digital humanities are encouraged to start with our introductory course, \"Programming Humanity.\" Advanced courses include \"A.I. for the Humanities.\" All digital humanities courses are project-based, and students will have a portfolio of innovative projects by the end of their course of study. Learn more at Kenyon Digital Humanities Website","title":"Digital Humanities and DHColab"},{"location":"api/","text":"Week: APIs Overview Data is the Alpha and Omega of Data Science, Machine Learning and Deep Learning. The creativity, expressiveness and strength of your analysis will be bounded by the data you have. The quality of your dataset can be measured along many dimensions including the type/number of features, originality, number of datapoints, accuracy, coherence, etc. While there are a rapidly growing number of public datasets, they often are best used for tutorials, training and establishing baseline metrics. It is extremely difficult to create new, innovative and meaningful data analysis based upon datasets already mined by others. This week we study Web Scraping and APIs so you will be able to create your own unique datasets based upon the wide variety and vast quantity of information available on the Web. Data constantly flows constantly in networks under two basic paradigms . First, Information intended for humans exit computer networks via a interactive and/or visual interfaces. These include (a) a widows-based Graphical User Interfaces (GUIs) like those in MacOS and WindowsOS, and (b) a text-based interactive REPL command line shell (read-execute-print-loop) . Machine-to-Human GUIs are carefully designed to represent, architect, and sequence information to leverage human intuitions and cognitive processes. Unfortunately, programmatically scraping data from web GUIs can be slow, error-prone, fragile and (in many cases) impossible due to the limited resources, continual design updates, technical anti-scraping barriers and legal concerns. Web scraping can be viable for creating unique datasets based upon simpler open web sites or for harvesting smaller high-value data. To web scrape effectively, you will need to learn several core concepts. Content (HTML) and presentation (CSS) metatags organize data within a web page. Specific data within a web page can be uniquely identified and extracted using XPath notation and Beautiful Soup 4. Web frameworks and designs change frequently, are often deeply nested in human-unfriendly fashion and follow no universal pattern which makes web scraping a constant challenge. Secondly, machine-to-machine API communications at the application layer are governed by strictly defined protocols like REST and GraphQL and exchange data in well-structured file formats like JSON and XML. Machine-to-machine communications have the advantages of speed, scalability and programmable automation. Many websites make their data available via open and/or paid access using their own API. In stark contrast to web pages, APIs and data exchange formats are (mostly) precisely defined by specificiations created by centralized standards committees. These standards rarely change and share universal best practices which makes exchanging data or creating unique datasets via APIs fast, efficient and reliable. This week introduces common web scraping tools and programming practices. It is particularly useful to learn this in a collaborative lab environment since there are so many rules and exceptions to the rule it can be hard to quickly get a functional overview. We\u2019ll also practice automating calls to popular REST API services to compile unique datasets. This includes learning the API for Twitter, Reddit and Instagram as well as the libraries to convert between JSON datafiles and internal Python data types like dicts{} and Pandas DataFrames. Applications Make sure you completed last week's DataCamp: - Intermediate Importing Data in Python (2hrs) Chp 1-4 [Monday]: (last Friday's Video) BA 322 | Scrape Zappos with Chrome Extension: Free Web Scraper (27:50) Find Website and Scrape with Chrome Plugin, esp with Geolocation data (e.g. MarineTraffic.com ) and scrape with Chrome Extension Visual Studio Code 2022 (19:36) Virtual Environments in Python - Crash Course (13:32) Presentation Bring Website to Scrape, with Geo Longitude/Latitude if possible. [Wednesday]: Step-by-Step Guide to Making Your First Request to the New Twitter API ) Twitter API with Python (Complete Guide) GitHub Tutorial - Beginner's Training Guide (8:11) Git - The Simple Guide [Friday]: (From Last Week) Automated Twitter Bot in Python (19:02) How to use JSON in Python (6:10) Python MySQL Tutorial - Setup & Basics (13:09) MySQL - The Basics (17:16) How to Save Data to MySQL Database (23:42) How to use the Twitter API v2 in Python using Tweepy (43:28) Software to Install MySQL Server for MacOS (MySQL Community Server) or Windows (MySQL Installer for Windows) - Don't need Oracle Account, Select 'Developer Default' VSCode Extension: MySQL by Jun Han (For Windows Only) Windows Subsystem for Linux In-Class Lab Setup Virtual Environments w/VS Code Scrape Twitter Manipulate w/JSON Save to MySQL Database SQLite3 Database","title":"Week: APIs"},{"location":"api/#week-apis","text":"","title":"Week: APIs"},{"location":"api/#overview","text":"Data is the Alpha and Omega of Data Science, Machine Learning and Deep Learning. The creativity, expressiveness and strength of your analysis will be bounded by the data you have. The quality of your dataset can be measured along many dimensions including the type/number of features, originality, number of datapoints, accuracy, coherence, etc. While there are a rapidly growing number of public datasets, they often are best used for tutorials, training and establishing baseline metrics. It is extremely difficult to create new, innovative and meaningful data analysis based upon datasets already mined by others. This week we study Web Scraping and APIs so you will be able to create your own unique datasets based upon the wide variety and vast quantity of information available on the Web. Data constantly flows constantly in networks under two basic paradigms . First, Information intended for humans exit computer networks via a interactive and/or visual interfaces. These include (a) a widows-based Graphical User Interfaces (GUIs) like those in MacOS and WindowsOS, and (b) a text-based interactive REPL command line shell (read-execute-print-loop) . Machine-to-Human GUIs are carefully designed to represent, architect, and sequence information to leverage human intuitions and cognitive processes. Unfortunately, programmatically scraping data from web GUIs can be slow, error-prone, fragile and (in many cases) impossible due to the limited resources, continual design updates, technical anti-scraping barriers and legal concerns. Web scraping can be viable for creating unique datasets based upon simpler open web sites or for harvesting smaller high-value data. To web scrape effectively, you will need to learn several core concepts. Content (HTML) and presentation (CSS) metatags organize data within a web page. Specific data within a web page can be uniquely identified and extracted using XPath notation and Beautiful Soup 4. Web frameworks and designs change frequently, are often deeply nested in human-unfriendly fashion and follow no universal pattern which makes web scraping a constant challenge. Secondly, machine-to-machine API communications at the application layer are governed by strictly defined protocols like REST and GraphQL and exchange data in well-structured file formats like JSON and XML. Machine-to-machine communications have the advantages of speed, scalability and programmable automation. Many websites make their data available via open and/or paid access using their own API. In stark contrast to web pages, APIs and data exchange formats are (mostly) precisely defined by specificiations created by centralized standards committees. These standards rarely change and share universal best practices which makes exchanging data or creating unique datasets via APIs fast, efficient and reliable. This week introduces common web scraping tools and programming practices. It is particularly useful to learn this in a collaborative lab environment since there are so many rules and exceptions to the rule it can be hard to quickly get a functional overview. We\u2019ll also practice automating calls to popular REST API services to compile unique datasets. This includes learning the API for Twitter, Reddit and Instagram as well as the libraries to convert between JSON datafiles and internal Python data types like dicts{} and Pandas DataFrames.","title":"Overview"},{"location":"api/#applications","text":"Make sure you completed last week's DataCamp: - Intermediate Importing Data in Python (2hrs) Chp 1-4 [Monday]: (last Friday's Video) BA 322 | Scrape Zappos with Chrome Extension: Free Web Scraper (27:50) Find Website and Scrape with Chrome Plugin, esp with Geolocation data (e.g. MarineTraffic.com ) and scrape with Chrome Extension Visual Studio Code 2022 (19:36) Virtual Environments in Python - Crash Course (13:32) Presentation Bring Website to Scrape, with Geo Longitude/Latitude if possible. [Wednesday]: Step-by-Step Guide to Making Your First Request to the New Twitter API ) Twitter API with Python (Complete Guide) GitHub Tutorial - Beginner's Training Guide (8:11) Git - The Simple Guide [Friday]: (From Last Week) Automated Twitter Bot in Python (19:02) How to use JSON in Python (6:10) Python MySQL Tutorial - Setup & Basics (13:09) MySQL - The Basics (17:16) How to Save Data to MySQL Database (23:42) How to use the Twitter API v2 in Python using Tweepy (43:28)","title":"Applications"},{"location":"api/#software-to-install","text":"MySQL Server for MacOS (MySQL Community Server) or Windows (MySQL Installer for Windows) - Don't need Oracle Account, Select 'Developer Default' VSCode Extension: MySQL by Jun Han (For Windows Only) Windows Subsystem for Linux","title":"Software to Install"},{"location":"api/#in-class-lab","text":"Setup Virtual Environments w/VS Code Scrape Twitter Manipulate w/JSON Save to MySQL Database SQLite3 Database","title":"In-Class Lab"},{"location":"diachronic_sa/","text":"Week: Diachronic Sentiment Analysis Overview Cultural analytics is the study of society and social phenomena by analyzing data and the way it flows. This course presumes some coding experience or the introductory course to Digital Humanities, Programming Humanity. We\u2019ll build on our skills using API\u2019s to create original datasets from social media sites like Twitter. Then we\u2019ll develop natural language processing skills including sentiment analysis and topic clustering to explore text for insights. We\u2019ll also learn how to graph and explore social networks. In class, we\u2019ll do some hands-on projects like analyzing the social network of Game of Thrones and trying to classify who\u2019s tweeting: Trump or Trudeau. In the final segment of the course, students develop their own project centered on their interests. Applications [Monday]: Mining the Dispatch [Wednesday]: The Largest Vocabulary in Hip Hop [Friday]: Fan Engagement Meter Coding Practice DataCamp Manipulating Time Series Data in Python Lab Prep GeoPy Lab Assignment Find at least 2 disparate shape/location datafiles and plot them both onto the same map using GeoPy.","title":"Week: Diachronic Sentiment Analysis"},{"location":"diachronic_sa/#week-diachronic-sentiment-analysis","text":"","title":"Week: Diachronic Sentiment Analysis"},{"location":"diachronic_sa/#overview","text":"Cultural analytics is the study of society and social phenomena by analyzing data and the way it flows. This course presumes some coding experience or the introductory course to Digital Humanities, Programming Humanity. We\u2019ll build on our skills using API\u2019s to create original datasets from social media sites like Twitter. Then we\u2019ll develop natural language processing skills including sentiment analysis and topic clustering to explore text for insights. We\u2019ll also learn how to graph and explore social networks. In class, we\u2019ll do some hands-on projects like analyzing the social network of Game of Thrones and trying to classify who\u2019s tweeting: Trump or Trudeau. In the final segment of the course, students develop their own project centered on their interests.","title":"Overview"},{"location":"diachronic_sa/#applications","text":"[Monday]: Mining the Dispatch [Wednesday]: The Largest Vocabulary in Hip Hop [Friday]: Fan Engagement Meter","title":"Applications"},{"location":"diachronic_sa/#coding-practice","text":"DataCamp Manipulating Time Series Data in Python","title":"Coding Practice"},{"location":"diachronic_sa/#lab-prep","text":"GeoPy","title":"Lab Prep"},{"location":"diachronic_sa/#lab-assignment","text":"Find at least 2 disparate shape/location datafiles and plot them both onto the same map using GeoPy.","title":"Lab Assignment"},{"location":"images/","text":"Week: Images Overview After several weeks of taking a deep dive into web scraping, APIs and learning professional development tools we bring these skills together. This week we'll begin to scrape, clean and analyze both natural text and images. We'll learn how to process the text and images we scrape from the web, request via REST APIs and obtain from pre-existing datasets. Applications [Monday]: Social Media Scraping w/Python Repo Datacamp: Image Processing in Python [Wednesday]: Gallery-dl Gallery-dl Video Directions Datacamp: Introduction to NLP [Friday]: How to Scrape Reddit with Python (2018) : An exercises in getting old code to work 13 Ways To Scrape Any Public Data From Any Website : Great overview to unify and contextualize some of the Scraping/API methods studied so far (Peruse) Surfing the Data Pipeline with Python : Another simplified/unified perspective on our weeks of Scraping/APIs FreeCodeCamp: Python Scraping Tutorial - tweepy & snscrape : Be judicious, pick the right tool for right job Resources reddit.com/prefs/apps PRAW API : Most popular Python API wrapper for Reddit skimage API : GOFAI/ML processing of images scraperr Library : Scrape Reddit Images/Galleries RedDownloader : Scrape Reddit Images w/o API snscrape Library : (Multiple) Social Network Scraper scweet Library : A new API-free twitter scraper (+images) Python Lib: user_agent MS BI Dashboard: Scrape Twitter for Sentiment Analysis : Twitter scraping + NLP/sentiment Analysis + Visualization Playwright : Web crawler automation engine (alternative to Selenium)","title":"Week: Images"},{"location":"images/#week-images","text":"","title":"Week: Images"},{"location":"images/#overview","text":"After several weeks of taking a deep dive into web scraping, APIs and learning professional development tools we bring these skills together. This week we'll begin to scrape, clean and analyze both natural text and images. We'll learn how to process the text and images we scrape from the web, request via REST APIs and obtain from pre-existing datasets.","title":"Overview"},{"location":"images/#applications","text":"[Monday]: Social Media Scraping w/Python Repo Datacamp: Image Processing in Python [Wednesday]: Gallery-dl Gallery-dl Video Directions Datacamp: Introduction to NLP [Friday]: How to Scrape Reddit with Python (2018) : An exercises in getting old code to work 13 Ways To Scrape Any Public Data From Any Website : Great overview to unify and contextualize some of the Scraping/API methods studied so far (Peruse) Surfing the Data Pipeline with Python : Another simplified/unified perspective on our weeks of Scraping/APIs FreeCodeCamp: Python Scraping Tutorial - tweepy & snscrape : Be judicious, pick the right tool for right job","title":"Applications"},{"location":"images/#resources","text":"reddit.com/prefs/apps PRAW API : Most popular Python API wrapper for Reddit skimage API : GOFAI/ML processing of images scraperr Library : Scrape Reddit Images/Galleries RedDownloader : Scrape Reddit Images w/o API snscrape Library : (Multiple) Social Network Scraper scweet Library : A new API-free twitter scraper (+images) Python Lib: user_agent MS BI Dashboard: Scrape Twitter for Sentiment Analysis : Twitter scraping + NLP/sentiment Analysis + Visualization Playwright : Web crawler automation engine (alternative to Selenium)","title":"Resources"},{"location":"kaggle_geospatial_course_questions/","text":"Cultural Analytics Class Outlines Week 2: Maps Wednesday: Questions on Kaggle Notebooks Kaggle Geospatial Course: https://www.kaggle.com/learn/geospatial-analysis Your First Map (Be sure to click on the black (<>) icon under the right \u201cExercise\u201d column) What does the library learntools do ( README.md file)? Uncomment q_1.[hint() and solution()] Find what 3 datasets are available in the Geopandas API What info does the dataset naturalearth_lowres have? What does fig, ax = plt.subplot(1, 2) do? Video explanation1 What is the difference between: PHL_loans = world_loans[world_loans['country'] == 'Philippines'] PHL_loans = world_loans.loc[world_loans.country == \"Philippines\"].copy() What does the library fiona do? What is the file format KML below mean? gpd.io.file.fiona.drvsupport.supported_drivers['KML'] = 'rw' Done Coordinate Reference System What does the library shapley do? What is the purpose of the argument \u201cparse_dates=\u201d below? birds_df = pd.read_csv(\"../input/geospatial-learn-course-data/purple_martin.csv\", parse_dates=['timestamp']) Find the manual page for GeoDataFrame.CRS ()? What is {\u2018init\u2019: \u2018 epsg:4326 \u2019}? What does: \u201cax.set_xlim([-110, -30])\u201d do? Parse these statements: path_df = birds. groupby (\"tag-local-identifier\")['geometry'] .apply(list) .apply(lambda x: LineString(x)).reset_index() start_df = birds.groupby(\"tag-local-identifier\")['geometry'].apply(list) .apply(lambda x: x[0]) .reset_index() end_df = birds.groupby(\"tag-local-identifier\")['geometry'].apply(list).apply(lambda x: x[-1] ).reset_index() Parse this statement: totalArea = sum(south_america.geometry .to_crs(epsg=3035) .area) / 10**6 protected_areas[protected_areas['MARINE']!='2'] .plot(ax=ax , alpha=0.4 , zorder=1) birds[birds.geometry.y < 0] .plot(ax=ax , color = 'red', alpha = 0.6, markersize = 10, zorder=2 ) (is higher zorder top or bottom?) Done Interactive Maps What does the library folium do? What does this statement mean? from IPython.display import IFrame What does the geometry column look like in plate_boundaries ? Break up the cell and inspect the plate_boundaries GeoDataFrame. Another way to see this geometry column before dropping it? Experiment with the radius i argument in the call to the HeatMap() method HeatMap(data=earthquakes[['Latitude', 'Longitude']], radius=15 ).add_to(m_1) What critical assumption does the join statement make (potential source of bugs)? stats = population.join(area_sqkm) What does the GeoDataFrame. geo_interface do? Explain how a folium choropleth map works? Done Manipulating Geospatial Data Questions in class Proximity Analysis Questions in class","title":"Kaggle geospatial course questions"},{"location":"mapping/","text":"Week: Mapping Overview In this course, the world of geospatial analytics is perhaps the most narrowly focused in purpose yet highly fragmented in term of methodology. Although maps can be used to tell complex and compelling stories, the workflow for generating maps themselves is relatively straightforward: create a basemap and add one or more informative layers. This is the same whether using advanced mapping software with drag-n-drop, no-code functionality (e.g. ArcGIS, Tableau or QGIS) or programming with popular mapping libraries (e.g. Leaflet.js, GeoPandas) The world of geospatial analytics is vast. It ranges from no-code/low-code Tableau, ArcGIS and QGIS to unique visualizations realized through customized Python/JavaScript code building-upon geospatial resources like OpenMap. Although we'll introduce some of the breath of geospatial analysis, our coding will focus on two popular Python libraries for geospatial mapping: GeoPandas and GeoPy. Applications [Monday] Torn Apart Separado, Summary Torn Apart Separado, Project [Wednesday] Not Even the Past: Social Vunerability and the Legacy of Redlining [Friday] Land Acquisition and Dispossession: Mapping the Homestead Act, 1863-1912 Coding Practice [Monday] GeoPy Jupyter Notebook with Commentary (Read this Jupyter notebook for the big picture. We'll review the details and implement on Friday in lab) Datafile for GeoPy Notebook: ithica-places.csv Jupyter GeoPy Notebook Part 1: Working with Geospatial Data in Python (NOTE: This datacamp is a bit less clear than most because it assumes a few key bits of prior knowledge. Don't get hung up on any unstated assumptions, work through it as best you can, and bring any questions to class. We'll explain the finer points in class.) [Wednesday] Parts 2 & 3: Working with Geospatial Data in Python [Friday] Part 4: Working with Geospatial Data in Python","title":"Week: Mapping"},{"location":"mapping/#week-mapping","text":"","title":"Week: Mapping"},{"location":"mapping/#overview","text":"In this course, the world of geospatial analytics is perhaps the most narrowly focused in purpose yet highly fragmented in term of methodology. Although maps can be used to tell complex and compelling stories, the workflow for generating maps themselves is relatively straightforward: create a basemap and add one or more informative layers. This is the same whether using advanced mapping software with drag-n-drop, no-code functionality (e.g. ArcGIS, Tableau or QGIS) or programming with popular mapping libraries (e.g. Leaflet.js, GeoPandas) The world of geospatial analytics is vast. It ranges from no-code/low-code Tableau, ArcGIS and QGIS to unique visualizations realized through customized Python/JavaScript code building-upon geospatial resources like OpenMap. Although we'll introduce some of the breath of geospatial analysis, our coding will focus on two popular Python libraries for geospatial mapping: GeoPandas and GeoPy.","title":"Overview"},{"location":"mapping/#applications","text":"[Monday] Torn Apart Separado, Summary Torn Apart Separado, Project [Wednesday] Not Even the Past: Social Vunerability and the Legacy of Redlining [Friday] Land Acquisition and Dispossession: Mapping the Homestead Act, 1863-1912","title":"Applications"},{"location":"mapping/#coding-practice","text":"[Monday] GeoPy Jupyter Notebook with Commentary (Read this Jupyter notebook for the big picture. We'll review the details and implement on Friday in lab) Datafile for GeoPy Notebook: ithica-places.csv Jupyter GeoPy Notebook Part 1: Working with Geospatial Data in Python (NOTE: This datacamp is a bit less clear than most because it assumes a few key bits of prior knowledge. Don't get hung up on any unstated assumptions, work through it as best you can, and bring any questions to class. We'll explain the finer points in class.) [Wednesday] Parts 2 & 3: Working with Geospatial Data in Python [Friday] Part 4: Working with Geospatial Data in Python","title":"Coding Practice"},{"location":"miniproject_1/","text":"Week: Mini-Project #1 Overview Outside class this well we'll read the chapters of an upcoming book on Geospatial Data Analysis using Pyhon. Although some code is presented in later chapters, this is more an overview of geospatial analysis from a professional more focused on the real-world concerns of health policy than those of a typical programmer. The loquacious style facilitates a quick read for high-level concepts. Look for universal patterns across various geospatial libraries, identify common workflows and bring any questions that arise during your readings to our class meetings. There is no coding outside class this week (no Datacamp). In class this week, we'll work on code in several existing Jupyter notebooks. We'll also do several coding exercises merging, manipulating and presenting geospatial data. By the end of the week you should have both a broad theoretic background in Geospatial analysis as well as a core ability to create, manipulate and present maps using Python and popular Geospatial libraries. Woo-hoo! Readings [Monday]: Ch 1: Introduction to Geospatial Analytics from Python for Geospatial Data Analysis by Bonny P. McClain, O'Reilly Nov 2022 Ch 2: Essential Facilities for Spatial Analytics from Python for Geospatial Data Analysis by Bonny P. McClain, O'Reilly Nov 2022 Ch 8: Data Cleaning from Python for Geospatial Data Analysis by Bonny P. McClain, O'Reilly Nov 2022 [Wednesday]: Ch 3: QGIS: Python for Spatial Analytics from Python for Geospatial Data Analysis by Bonny P. McClain, O'Reilly Nov 2022 Ch 4: Geospatial Analytics in the Cloud: Google Earth Engine and Other Tools from Python for Geospatial Data Analysis by Bonny P. McClain, O'Reilly Nov 2022 Ch 5: Open StreetMap: Accessing Geospatial Data with OSMnx from Python for Geospatial Data Analysis by Bonny P. McClain, O'Reilly Nov 2022 [Friday]: Complete Kaggle Geospatial Course, Notebooks 3-5 ~~Ch 6: ArcGIS Python API from Python for Geospatial Data Analysis by Bonny P. McClain, O'Reilly Nov 2022~~ (Optional) Ch 7: GeoPandas and Spatial Statistics from Python for Geospatial Data Analysis by Bonny P. McClain, O'Reilly Nov 2022 Coding Practice in Class Datacamp.com Working with Geospatial Data in Python (4hrs) [Monday]: Finding and Creating Geospatial Datasets Jupyter Notebook #1: Leaflet Key Features Jupyter Notebook #1: OSMnx [Wednesday]: Basics of Geospatial Analysis Kaggle Course Kaggle Course Questions Kaggle Geopandas Notebooks [Friday]: Presenting to the World Create your own Geospatial visualization in a Jupyter Notebook from scratch Mapping Resources A Beginner's Guide to Geospatial Data Analysis A concise summary of the 7 most important functions in GeoPandas Taxonomy of Geospatial Analytics Resources for Mapping Mini-Project Brainstorming Brainstorming Team Assignments","title":"Week: Mini-Project #1"},{"location":"miniproject_1/#week-mini-project-1","text":"","title":"Week: Mini-Project #1"},{"location":"miniproject_1/#overview","text":"Outside class this well we'll read the chapters of an upcoming book on Geospatial Data Analysis using Pyhon. Although some code is presented in later chapters, this is more an overview of geospatial analysis from a professional more focused on the real-world concerns of health policy than those of a typical programmer. The loquacious style facilitates a quick read for high-level concepts. Look for universal patterns across various geospatial libraries, identify common workflows and bring any questions that arise during your readings to our class meetings. There is no coding outside class this week (no Datacamp). In class this week, we'll work on code in several existing Jupyter notebooks. We'll also do several coding exercises merging, manipulating and presenting geospatial data. By the end of the week you should have both a broad theoretic background in Geospatial analysis as well as a core ability to create, manipulate and present maps using Python and popular Geospatial libraries. Woo-hoo!","title":"Overview"},{"location":"miniproject_1/#readings","text":"[Monday]: Ch 1: Introduction to Geospatial Analytics from Python for Geospatial Data Analysis by Bonny P. McClain, O'Reilly Nov 2022 Ch 2: Essential Facilities for Spatial Analytics from Python for Geospatial Data Analysis by Bonny P. McClain, O'Reilly Nov 2022 Ch 8: Data Cleaning from Python for Geospatial Data Analysis by Bonny P. McClain, O'Reilly Nov 2022 [Wednesday]: Ch 3: QGIS: Python for Spatial Analytics from Python for Geospatial Data Analysis by Bonny P. McClain, O'Reilly Nov 2022 Ch 4: Geospatial Analytics in the Cloud: Google Earth Engine and Other Tools from Python for Geospatial Data Analysis by Bonny P. McClain, O'Reilly Nov 2022 Ch 5: Open StreetMap: Accessing Geospatial Data with OSMnx from Python for Geospatial Data Analysis by Bonny P. McClain, O'Reilly Nov 2022 [Friday]: Complete Kaggle Geospatial Course, Notebooks 3-5 ~~Ch 6: ArcGIS Python API from Python for Geospatial Data Analysis by Bonny P. McClain, O'Reilly Nov 2022~~ (Optional) Ch 7: GeoPandas and Spatial Statistics from Python for Geospatial Data Analysis by Bonny P. McClain, O'Reilly Nov 2022","title":"Readings"},{"location":"miniproject_1/#coding-practice-in-class","text":"Datacamp.com Working with Geospatial Data in Python (4hrs) [Monday]: Finding and Creating Geospatial Datasets Jupyter Notebook #1: Leaflet Key Features Jupyter Notebook #1: OSMnx [Wednesday]: Basics of Geospatial Analysis Kaggle Course Kaggle Course Questions Kaggle Geopandas Notebooks [Friday]: Presenting to the World Create your own Geospatial visualization in a Jupyter Notebook from scratch","title":"Coding Practice in Class"},{"location":"miniproject_1/#mapping-resources","text":"A Beginner's Guide to Geospatial Data Analysis A concise summary of the 7 most important functions in GeoPandas Taxonomy of Geospatial Analytics Resources for Mapping","title":"Mapping Resources"},{"location":"miniproject_1/#mini-project-brainstorming","text":"Brainstorming Team Assignments","title":"Mini-Project Brainstorming"},{"location":"miniproject_2/","text":"Week: Mini-Project #2 UPDATE Click on image above to watch Midjourney AI-generated video using lyrics to \"Don't Stop Me Now\" by Queen Overview This week we bring together all the many disparate and not-so-disparate skills honed over the past several weeks into Mini-Project #2. Over the past few months, text to image (and now video) generation has seen a dramatic leap forward with the introduction of new large AI models. This also aligns nicely with our own AI research, so we can lend an unusual degree of experience to this rapidly evolving field. Creating text prompts (prompt engineering) to feed into these text2image models is one of the hottest areas of AI research recently, and we'll explore this further this week. This involves an all-hands-on-deck class project that decomposes nicely into smaller groups: text to image generation by recently released state-of-the-art large DNN models. GOAL: Research, search and scrape Twitter for images and prompts based upon the following 3 state-of-the-art text2image deep neural network (DNN) models: DALL-E 2 : @openai Midjourney: @midjourney Stable Diffusion : @stablediffusion Our goal is to use what we've learned about scraping, APIs, NLP and image processing to find tweets with text generated images on Twitter. In particular, we want to collect the following: the author/organization text prompt with the associated generated image any other relevant information, comments or meta-information Everyone will be assigned into one of three groups representing each of the main three text2image DNN models listed above. Each group will independently research, scrape and analyze as much data as they can for their assigned DNN model. Then as a group, we will combine, compare, and critique our findings as a unified team. Readings [Monday]: The absolute beginners guide to MidJourney AI. Starting with AI Art (start at 20:00 and peruse for a 5-10 minutes to get a sense of how to interactively design prompt for image generation) Research all 3 models including (start with the links at the top of this page under GOALS) Websites (official and tutorials) Reddit subthreads (r/subreddits) Twitter (official and tutorials) Twitter (artists and programmers) The Mind-Blowing DALL\u00b7E 2 Prompt Book Expand and read the description immediately below the YouTube video for links to various text2image social media users and groups. DALL-e Prompt Book Review Twitter API (version 2) from previous weeks [Wednesday]: Design Guidelines for Prompt Engineering Text-to-Image Generative Models (7:33) Brief overview of ACM paper on prompt engineering experiments (Gallery) Lexica.art Scroll through, search and roll-over to view text prompt that generated each image (Gallery) PromptHero.com (Gallery) NightCafe.Studio [Friday]: Lexica.art: API Instructions Freecodecamp.com: Web Scraping with Python, Tweepy and Snscrape Github Action to Scrape #depression Tweets Daily How to Scrape Twitter with Snscrape with Repo scrape_twitter_7day.py - For Normal dev accounts that can only scrape over the last 7 days (update the start/end date vars to correspond to a current trailing 7day window) scrape_twitter_freecc.py For academic research dev accounts that can scrape beyond 7 days. scrape_reddit_sentiment.py Simple Reddit Scraper Assignments: Teams: Social Media DALL-E 2 Midjourney Stable Diffusion Twitter Freddie Jill Jeremy Ani Claire Vikas Reddit Viet Max Devon Teddy Anav Abbie Tao Identify Scrape Targets: * Official Social Media Accounts (e.g. @handles) * Groups/Boards of Fans (e.g. subreddits) * Search Terms (e.g. regular or #hashtags) * Individual Artists * Gallaries (e.g. Lexica.art) * Other sources (e.g. Slack, Discord) Guidance: * Keep good notes as you go * Give FULL DETAILS we need to scrape BOTH (a) Generated Images and (b) text prompts (+ possible explainations/given context) - Full URL paths/subdirectories - Unique usernames/handles/hashtags - Distinct and effective keywords/search terms - etc * Try to group common patterns into a taxonomy as you to, make notes on what distinctive features you are using to base your classification on References unicodedata library tweets_df['Tweet'] = tweets_df['Tweet'].apply(lambda x: unicodedata.normalize('NFD', x).encode('ascii', 'ignore')) Git - The Simple Guide Promptbase: A marketplace for text engineering Awesome Prompt Papers Python os.environ() vs python-dotenv Twitter Hashtag Search: keywordtool Twitter Hashtag Search: tagsfinder Twitter API Ver 2 Twitter Developer Platform Resources An Extenstive Guide to Collecting Tweets from Twitter API v2 for Academic Research Using Python 3 (Academic 10M/mo access level) A Comprehensive Guide for Using the Twitter API v2 Using Tweepy Twitter API Ver 2: Reference Twitter API Ver 2: Playground Twitter API Ver 2: Query Builder Twitter API Ver 2 Annotations: Entities.Context Twitter API Ver 2 Data Dictionary Twitter API Ver 2: Examples Twitter API Ver 2: Sample Code Twitter API Ver 2: Notification via Integration with AWS/Twilio (Java) Sentiment Analysis of Live Tweets (12:00) 3/3 Discord Awesome Discord Communities Discord Official API Discord API Python Wrapper Discord RedBot PyDiscord Bot DiscordChatExporter (Win GUI/CLI) Slack Awesome Slack (3/22) Slack Web Scraper Github + Slack Integration Official Slack API Other Scrapers & Bots Awesome Bots (8/21) Rasa Multichat Bot ML Automation Mattermost Multichat Bridge disease-sh API and c19 scraper (JS) Wayback Machine Scraper (CL) Code Samples Repl.it Glitch.com Hitomi Scripts Twitter API ver 2 Sample Code Elon Musk Tweet Auto Scraper Books Mastering API Architecture Docker Deep Dive ML Engineering AWS","title":"Week: Mini-Project #2"},{"location":"miniproject_2/#week-mini-project-2","text":"","title":"Week: Mini-Project #2"},{"location":"miniproject_2/#update","text":"Click on image above to watch Midjourney AI-generated video using lyrics to \"Don't Stop Me Now\" by Queen","title":"UPDATE"},{"location":"miniproject_2/#overview","text":"This week we bring together all the many disparate and not-so-disparate skills honed over the past several weeks into Mini-Project #2. Over the past few months, text to image (and now video) generation has seen a dramatic leap forward with the introduction of new large AI models. This also aligns nicely with our own AI research, so we can lend an unusual degree of experience to this rapidly evolving field. Creating text prompts (prompt engineering) to feed into these text2image models is one of the hottest areas of AI research recently, and we'll explore this further this week. This involves an all-hands-on-deck class project that decomposes nicely into smaller groups: text to image generation by recently released state-of-the-art large DNN models. GOAL: Research, search and scrape Twitter for images and prompts based upon the following 3 state-of-the-art text2image deep neural network (DNN) models: DALL-E 2 : @openai Midjourney: @midjourney Stable Diffusion : @stablediffusion Our goal is to use what we've learned about scraping, APIs, NLP and image processing to find tweets with text generated images on Twitter. In particular, we want to collect the following: the author/organization text prompt with the associated generated image any other relevant information, comments or meta-information Everyone will be assigned into one of three groups representing each of the main three text2image DNN models listed above. Each group will independently research, scrape and analyze as much data as they can for their assigned DNN model. Then as a group, we will combine, compare, and critique our findings as a unified team.","title":"Overview"},{"location":"miniproject_2/#readings","text":"[Monday]: The absolute beginners guide to MidJourney AI. Starting with AI Art (start at 20:00 and peruse for a 5-10 minutes to get a sense of how to interactively design prompt for image generation) Research all 3 models including (start with the links at the top of this page under GOALS) Websites (official and tutorials) Reddit subthreads (r/subreddits) Twitter (official and tutorials) Twitter (artists and programmers) The Mind-Blowing DALL\u00b7E 2 Prompt Book Expand and read the description immediately below the YouTube video for links to various text2image social media users and groups. DALL-e Prompt Book Review Twitter API (version 2) from previous weeks [Wednesday]: Design Guidelines for Prompt Engineering Text-to-Image Generative Models (7:33) Brief overview of ACM paper on prompt engineering experiments (Gallery) Lexica.art Scroll through, search and roll-over to view text prompt that generated each image (Gallery) PromptHero.com (Gallery) NightCafe.Studio [Friday]: Lexica.art: API Instructions Freecodecamp.com: Web Scraping with Python, Tweepy and Snscrape Github Action to Scrape #depression Tweets Daily How to Scrape Twitter with Snscrape with Repo scrape_twitter_7day.py - For Normal dev accounts that can only scrape over the last 7 days (update the start/end date vars to correspond to a current trailing 7day window) scrape_twitter_freecc.py For academic research dev accounts that can scrape beyond 7 days. scrape_reddit_sentiment.py Simple Reddit Scraper","title":"Readings"},{"location":"miniproject_2/#assignments","text":"Teams: Social Media DALL-E 2 Midjourney Stable Diffusion Twitter Freddie Jill Jeremy Ani Claire Vikas Reddit Viet Max Devon Teddy Anav Abbie Tao Identify Scrape Targets: * Official Social Media Accounts (e.g. @handles) * Groups/Boards of Fans (e.g. subreddits) * Search Terms (e.g. regular or #hashtags) * Individual Artists * Gallaries (e.g. Lexica.art) * Other sources (e.g. Slack, Discord) Guidance: * Keep good notes as you go * Give FULL DETAILS we need to scrape BOTH (a) Generated Images and (b) text prompts (+ possible explainations/given context) - Full URL paths/subdirectories - Unique usernames/handles/hashtags - Distinct and effective keywords/search terms - etc * Try to group common patterns into a taxonomy as you to, make notes on what distinctive features you are using to base your classification on","title":"Assignments:"},{"location":"miniproject_2/#references","text":"unicodedata library tweets_df['Tweet'] = tweets_df['Tweet'].apply(lambda x: unicodedata.normalize('NFD', x).encode('ascii', 'ignore')) Git - The Simple Guide Promptbase: A marketplace for text engineering Awesome Prompt Papers Python os.environ() vs python-dotenv Twitter Hashtag Search: keywordtool Twitter Hashtag Search: tagsfinder","title":"References"},{"location":"miniproject_2/#twitter-api-ver-2","text":"Twitter Developer Platform Resources An Extenstive Guide to Collecting Tweets from Twitter API v2 for Academic Research Using Python 3 (Academic 10M/mo access level) A Comprehensive Guide for Using the Twitter API v2 Using Tweepy Twitter API Ver 2: Reference Twitter API Ver 2: Playground Twitter API Ver 2: Query Builder Twitter API Ver 2 Annotations: Entities.Context Twitter API Ver 2 Data Dictionary Twitter API Ver 2: Examples Twitter API Ver 2: Sample Code Twitter API Ver 2: Notification via Integration with AWS/Twilio (Java) Sentiment Analysis of Live Tweets (12:00) 3/3","title":"Twitter API Ver 2"},{"location":"miniproject_2/#discord","text":"Awesome Discord Communities Discord Official API Discord API Python Wrapper Discord RedBot PyDiscord Bot DiscordChatExporter (Win GUI/CLI)","title":"Discord"},{"location":"miniproject_2/#slack","text":"Awesome Slack (3/22) Slack Web Scraper Github + Slack Integration Official Slack API","title":"Slack"},{"location":"miniproject_2/#other-scrapers-bots","text":"Awesome Bots (8/21) Rasa Multichat Bot ML Automation Mattermost Multichat Bridge disease-sh API and c19 scraper (JS) Wayback Machine Scraper (CL)","title":"Other Scrapers &amp; Bots"},{"location":"miniproject_2/#code-samples","text":"Repl.it Glitch.com Hitomi Scripts Twitter API ver 2 Sample Code Elon Musk Tweet Auto Scraper","title":"Code Samples"},{"location":"miniproject_2/#books","text":"Mastering API Architecture Docker Deep Dive ML Engineering AWS","title":"Books"},{"location":"ml/","text":"","title":"Ml"},{"location":"networks/","text":"Week: Social Networks Overview This week you'll learn the basics of network analysis and the popular Python library NetworkX. We'll use graphs to model character interactions in Game of Thrones as well as understand the social networks of US politicians on Twitter. We'll customize the social networks of US politicians Jupyter notebook for your own goals in a project that will integrate your Twitter API, NLP and network analysis skills. On Friday, we'll review code to see two other common social network analysis. We'll also peek beyond simple static network analysis to see two more sophisticated statistical ML and mathematical models based upon network data. Applications [Monday]: Analyzing Social Media Data in Python (Chapters 1-2) Network Game of Thones DH Twitter Graph Starter Game of Thrones: Network Analysis [Wednesday]: scrape_twitter_usgovs_simple - for scraping Followers/Followed per US state governor and combining them with 'source' and 'destination' renamed columns Analyzing Social Media Data in Python (Chapters 1-2) Postman.com Sign-up US Politicians Twitter Network Analysis WARNING: Offensive Language: Cyberbulling Twitter [Friday]: Github Social Network Analysis Chilean Artists NetworkX Predict Customers Next Purchase Day w/XGBoost RFM Can A Graph Help Classify Player Data Resources Twitter Developer Dashboard Twitter API v2 Sample Code Getting Started with the Twitter API v2 for Academic Research Reddit Authorized Apps Introduction to Social Network Analysis 1/5: Main Concepts (15:45) Introduction to Social Network Analysis [2/5]: The Origins (11:02) Network of Thrones Game of Thrones Summary (7:40) Game of Thrones, Jul 2016 D&D and G a daring tale of Dungeons and Dragons and also Graph (23:27) Programming with NetworkX in Python (24:12) Network Analysis with Python/NetworkX (20:40) Complex network analysis with NetworkX (UPTO 28:00) Four Common Ways to Visualize Graphs Docker on MacOS Docker on Win10+ Visualize Graph with AlgorithmX Widget in Jupyter Twitter Graph in Memgraph Coding Practice AlgorithmX Docs AlgorithmX Jupyter Notebook NetworkX Geospatial Examples Chinese Postman Problem (TSP Variant) Thanksgiving Break US Govs Notebook (old - previously assigned) Introduction to Network Analysis in Python (new for Thanksgiving break) Analyzing Social Media Data in Python What is REST API? Examples and How to Use Today Analyzing Text Similiarity Using SpaCy, networkx Network centrality using networkx NetworkX API Centrality Metrics - Review and come to next class with explainations and examples of applications Twitter Semantic Networks with snscrape Diachronic Sentiment Analysis Simplified SentimentArcs for Novels Simplified SentimentArcs for Tweets","title":"Week: Social Networks"},{"location":"networks/#week-social-networks","text":"","title":"Week: Social Networks"},{"location":"networks/#overview","text":"This week you'll learn the basics of network analysis and the popular Python library NetworkX. We'll use graphs to model character interactions in Game of Thrones as well as understand the social networks of US politicians on Twitter. We'll customize the social networks of US politicians Jupyter notebook for your own goals in a project that will integrate your Twitter API, NLP and network analysis skills. On Friday, we'll review code to see two other common social network analysis. We'll also peek beyond simple static network analysis to see two more sophisticated statistical ML and mathematical models based upon network data.","title":"Overview"},{"location":"networks/#applications","text":"[Monday]: Analyzing Social Media Data in Python (Chapters 1-2) Network Game of Thones DH Twitter Graph Starter Game of Thrones: Network Analysis [Wednesday]: scrape_twitter_usgovs_simple - for scraping Followers/Followed per US state governor and combining them with 'source' and 'destination' renamed columns Analyzing Social Media Data in Python (Chapters 1-2) Postman.com Sign-up US Politicians Twitter Network Analysis WARNING: Offensive Language: Cyberbulling Twitter [Friday]: Github Social Network Analysis Chilean Artists NetworkX Predict Customers Next Purchase Day w/XGBoost RFM Can A Graph Help Classify Player Data","title":"Applications"},{"location":"networks/#resources","text":"Twitter Developer Dashboard Twitter API v2 Sample Code Getting Started with the Twitter API v2 for Academic Research Reddit Authorized Apps Introduction to Social Network Analysis 1/5: Main Concepts (15:45) Introduction to Social Network Analysis [2/5]: The Origins (11:02) Network of Thrones Game of Thrones Summary (7:40) Game of Thrones, Jul 2016 D&D and G a daring tale of Dungeons and Dragons and also Graph (23:27) Programming with NetworkX in Python (24:12) Network Analysis with Python/NetworkX (20:40) Complex network analysis with NetworkX (UPTO 28:00) Four Common Ways to Visualize Graphs Docker on MacOS Docker on Win10+ Visualize Graph with AlgorithmX Widget in Jupyter Twitter Graph in Memgraph","title":"Resources"},{"location":"networks/#coding-practice","text":"AlgorithmX Docs AlgorithmX Jupyter Notebook NetworkX Geospatial Examples Chinese Postman Problem (TSP Variant)","title":"Coding Practice"},{"location":"networks/#thanksgiving-break","text":"US Govs Notebook (old - previously assigned) Introduction to Network Analysis in Python (new for Thanksgiving break) Analyzing Social Media Data in Python What is REST API? Examples and How to Use","title":"Thanksgiving Break"},{"location":"networks/#today","text":"Analyzing Text Similiarity Using SpaCy, networkx Network centrality using networkx NetworkX API Centrality Metrics - Review and come to next class with explainations and examples of applications Twitter Semantic Networks with snscrape","title":"Today"},{"location":"networks/#diachronic-sentiment-analysis","text":"Simplified SentimentArcs for Novels Simplified SentimentArcs for Tweets","title":"Diachronic Sentiment Analysis"},{"location":"nlp_advanced/","text":"Week: Advanced NLP Overview Despite the title, this week is a deeper dive into the theory, tools and practice building datasets based upon APIs. This includes a more exhaustive exploration of the new Twitter ver 2 API as well as overviews of APIs for Reddit and Instagram. We'll also become a bit more familiar with our new development environment and tools like VSCode, github and Python developer best practices. Applications [Monday]: Scrape Twitter's FREE v2 Data API (9:17) Web Scraping with Python, Everything You Need to Know 2022 Step-by-step guide to making your first request to the new Twitter API v2 Getting Started with Twitter (Upto/incl 'Make Your First Request') Getting Started with Twitter API ver 2 for Academic Research (DO ALL 8 Modules) [Wednesday]: Download and Install SQLite Browser Download and Install Insomnia Scrape LIVE scores - No BeautifulSoup or Selenium NEEDED! (15:43) * Scraping Reddit with PRAW (Python Reddit API Wrapper) Apr 2022 Simple Script: Reddit * ~~ Scraping Reddit with Python and Beautiful Soup 4 ~~ How to Scrape Reddit and Automatically Label Data for NLP (13:07) Twitter API v2 Sample Endpoints Twitter API v2 Tutorial: Advanced Filtering with GeoData Create & Manage Repositories (This section only) GitHub Fundamentals [Friday]: How to Scrape Everything from Instagram using Python Scraping every post on an Instagram profile with less than 10 lines of Python How to Fetch Data From Instagram Using Python How I Scrape Amazon with Python (14:33) Simple Instagram Bot in Python (13:03) Adding Locally Hosted Code to Github GraphQL Basics - Build an app with the SpaceX API (UPTO 6:00 ONLY) Coding Practice How to Scrape Instagram How to Use VSCode as Client to Test REST API (3:43) - settings-demo.json autocorrect Github Instagram Resources Simple Script: InstaPy Instaloader Amazon Product 4 way Scrapers Getting Started with Google APIs for Python Development (22:36) Google API Explorer","title":"Week: Advanced NLP"},{"location":"nlp_advanced/#week-advanced-nlp","text":"","title":"Week: Advanced NLP"},{"location":"nlp_advanced/#overview","text":"Despite the title, this week is a deeper dive into the theory, tools and practice building datasets based upon APIs. This includes a more exhaustive exploration of the new Twitter ver 2 API as well as overviews of APIs for Reddit and Instagram. We'll also become a bit more familiar with our new development environment and tools like VSCode, github and Python developer best practices.","title":"Overview"},{"location":"nlp_advanced/#applications","text":"[Monday]: Scrape Twitter's FREE v2 Data API (9:17) Web Scraping with Python, Everything You Need to Know 2022 Step-by-step guide to making your first request to the new Twitter API v2 Getting Started with Twitter (Upto/incl 'Make Your First Request') Getting Started with Twitter API ver 2 for Academic Research (DO ALL 8 Modules) [Wednesday]: Download and Install SQLite Browser Download and Install Insomnia Scrape LIVE scores - No BeautifulSoup or Selenium NEEDED! (15:43) * Scraping Reddit with PRAW (Python Reddit API Wrapper) Apr 2022 Simple Script: Reddit * ~~ Scraping Reddit with Python and Beautiful Soup 4 ~~ How to Scrape Reddit and Automatically Label Data for NLP (13:07) Twitter API v2 Sample Endpoints Twitter API v2 Tutorial: Advanced Filtering with GeoData Create & Manage Repositories (This section only) GitHub Fundamentals [Friday]: How to Scrape Everything from Instagram using Python Scraping every post on an Instagram profile with less than 10 lines of Python How to Fetch Data From Instagram Using Python How I Scrape Amazon with Python (14:33) Simple Instagram Bot in Python (13:03) Adding Locally Hosted Code to Github GraphQL Basics - Build an app with the SpaceX API (UPTO 6:00 ONLY)","title":"Applications"},{"location":"nlp_advanced/#coding-practice","text":"How to Scrape Instagram How to Use VSCode as Client to Test REST API (3:43) - settings-demo.json autocorrect Github Instagram","title":"Coding Practice"},{"location":"nlp_advanced/#resources","text":"Simple Script: InstaPy Instaloader Amazon Product 4 way Scrapers Getting Started with Google APIs for Python Development (22:36) Google API Explorer","title":"Resources"},{"location":"nlp_intro/","text":"Week: Introduction to NLP Overview NOTE: We will spend most of the 2-day week before October Break continuing to study Twitter, APIs, REST and Web Servers. Natural Language Processing (NLP) is the algorithmic processing of human written text and consists of a number of discrete/narrow tasks including: Optical Character Recognition (OCR) Word Tokenization, Sentence Boundary Detection and other Semantic Segmentation Part of Speech Tagging Sentiment Analysis Machine Translation Dialog/Chatbots Natural Language Generation ...and many more NLP is a fascinating field of study since language is intimately tied to much of which makes us human from our individual cognition to our social dynamics. Applications [Monday]: REST Wiki HTTP Crash Course & Exploration (upto 16:20) What is A RESTful API? (18:30) How to Use the Twitter API v2 in Python Using Tweepy (43:28) [Wednesday]: Thunder Client - VSCode Extension (9:01) Building a Simple REST API with FastAPI (37:11) Ultimate Guide to Understand and Implement Natural Language Processing [Friday]: October Break Coding Practice Twitter API with Python 2022 1/3 (9:14) Twitter API with Python 2022 2/3 (10:10) Twitter API with Python 2022 3/3 (12:00) Black Flake Embold References Twitter API v2 Calls Twitter Dev Resources Twitter API v2 Sample Code Twitter API Reddit API tweepy snscrape Swagger.io Reverse Engineering BeReal Hacking BeReal with a MITM Attack","title":"Week: Introduction to NLP"},{"location":"nlp_intro/#week-introduction-to-nlp","text":"","title":"Week: Introduction to NLP"},{"location":"nlp_intro/#overview","text":"NOTE: We will spend most of the 2-day week before October Break continuing to study Twitter, APIs, REST and Web Servers. Natural Language Processing (NLP) is the algorithmic processing of human written text and consists of a number of discrete/narrow tasks including: Optical Character Recognition (OCR) Word Tokenization, Sentence Boundary Detection and other Semantic Segmentation Part of Speech Tagging Sentiment Analysis Machine Translation Dialog/Chatbots Natural Language Generation ...and many more NLP is a fascinating field of study since language is intimately tied to much of which makes us human from our individual cognition to our social dynamics.","title":"Overview"},{"location":"nlp_intro/#applications","text":"[Monday]: REST Wiki HTTP Crash Course & Exploration (upto 16:20) What is A RESTful API? (18:30) How to Use the Twitter API v2 in Python Using Tweepy (43:28) [Wednesday]: Thunder Client - VSCode Extension (9:01) Building a Simple REST API with FastAPI (37:11) Ultimate Guide to Understand and Implement Natural Language Processing [Friday]: October Break","title":"Applications"},{"location":"nlp_intro/#coding-practice","text":"Twitter API with Python 2022 1/3 (9:14) Twitter API with Python 2022 2/3 (10:10) Twitter API with Python 2022 3/3 (12:00) Black Flake Embold","title":"Coding Practice"},{"location":"nlp_intro/#references","text":"Twitter API v2 Calls Twitter Dev Resources Twitter API v2 Sample Code Twitter API Reddit API tweepy snscrape Swagger.io Reverse Engineering BeReal Hacking BeReal with a MITM Attack","title":"References"},{"location":"project/","text":"Projects Overview Our course project will give you the opportunity to explore, learn and showcase applying one or more technical concepts/tools to a topic of your personal interest. The focus will generally be on interpretation, critique and creativity to demonstrating domain expertise by applying commonly available ML/AI models rather than coding. All projects will be submitted in the form of a presentation poster similar to many ML/AI courses in line with guidelines for Stanford University\u2019s CS230 project guidelines. The broad steps of your project are: Directions and Poster Templates Write up your project in standard research poster format: Follow the Header in this Template for IPHS290 Spring 2022 (48 x 36 size) A wide variety of PowerPoint Templates for your Poster (48 x 36 size) Research Poster Format Previous Project Examples For inspiration, see examples DH class project posters for previous years on Digital Kenyon .","title":"Project"},{"location":"project/#projects","text":"","title":"Projects"},{"location":"project/#overview","text":"Our course project will give you the opportunity to explore, learn and showcase applying one or more technical concepts/tools to a topic of your personal interest. The focus will generally be on interpretation, critique and creativity to demonstrating domain expertise by applying commonly available ML/AI models rather than coding. All projects will be submitted in the form of a presentation poster similar to many ML/AI courses in line with guidelines for Stanford University\u2019s CS230 project guidelines. The broad steps of your project are:","title":"Overview"},{"location":"project/#directions-and-poster-templates","text":"Write up your project in standard research poster format: Follow the Header in this Template for IPHS290 Spring 2022 (48 x 36 size) A wide variety of PowerPoint Templates for your Poster (48 x 36 size) Research Poster Format","title":"Directions and Poster Templates"},{"location":"project/#previous-project-examples","text":"For inspiration, see examples DH class project posters for previous years on Digital Kenyon .","title":"Previous Project Examples"},{"location":"readings/","text":"Readings Final Project Step 1: Sentiment Analysis of Tweets Colab notebook Step 2 (Optional): Topic Modeling for Tweets Colab notebook Step 2 can only read the datafile.csv (processed, formatted and saved/output) from the output of Step 1 Sentiment Analysis of Tweets Colab notebook Geospatial Visualizations Mapping Mini-Project #1: Mapping Due : before class Wednesday, Sep 21, 2022 (weeks 2 & 3) Mini-Project #1: Geospatial Analysis Data and Bots Web Scraping Scraping and APIs Overview Scraping in Depth APIs/Web in Depth Images and Text Mini-Project #2: Social Networks Due : before class Monday, October 31, 2022 Mini-Project #2: Scrape Twitter for AI generated Images and associated Text Prompts (week 10) Scrape Twitter to Social Network (CURRENT WEEK) Mini-Project #2 Resources Natural Language Processing (NLP) Sentiment Analysis Advanced NLP Topic Modeling Diachronic Sentiment Analysis Mini-Project #3: Diachronic Sentiment Analysis Mini-Project #3 Resources Sharing Content Web Servers Final Project: Poster Final Project Resources","title":"Readings"},{"location":"readings/#readings","text":"","title":"Readings"},{"location":"readings/#final-project","text":"Step 1: Sentiment Analysis of Tweets Colab notebook Step 2 (Optional): Topic Modeling for Tweets Colab notebook Step 2 can only read the datafile.csv (processed, formatted and saved/output) from the output of Step 1 Sentiment Analysis of Tweets Colab notebook","title":"Final Project"},{"location":"readings/#geospatial-visualizations","text":"Mapping","title":"Geospatial Visualizations"},{"location":"readings/#mini-project-1-mapping","text":"Due : before class Wednesday, Sep 21, 2022 (weeks 2 & 3) Mini-Project #1: Geospatial Analysis","title":"Mini-Project #1: Mapping"},{"location":"readings/#data-and-bots","text":"Web Scraping Scraping and APIs Overview Scraping in Depth APIs/Web in Depth Images and Text","title":"Data and Bots"},{"location":"readings/#mini-project-2-social-networks","text":"Due : before class Monday, October 31, 2022 Mini-Project #2: Scrape Twitter for AI generated Images and associated Text Prompts (week 10) Scrape Twitter to Social Network (CURRENT WEEK) Mini-Project #2 Resources","title":"Mini-Project #2: Social Networks"},{"location":"readings/#natural-language-processing-nlp","text":"Sentiment Analysis Advanced NLP Topic Modeling Diachronic Sentiment Analysis","title":"Natural Language Processing (NLP)"},{"location":"readings/#mini-project-3-diachronic-sentiment-analysis","text":"Mini-Project #3 Resources","title":"Mini-Project #3: Diachronic Sentiment Analysis"},{"location":"readings/#sharing-content","text":"Web Servers","title":"Sharing Content"},{"location":"readings/#final-project-poster","text":"Final Project Resources","title":"Final Project: Poster"},{"location":"resources/","text":"","title":"Resources"},{"location":"scraping/","text":"Week: Scraping Overview Data is the Alpha and Omega of Data Science, Machine Learning and Deep Learning. The creativity, expressiveness and strength of your analysis will be bounded by the data you have. The quality of your dataset can be measured along many dimensions including the type/number of features, originality, number of datapoints, accuracy, coherence, etc. While there are a rapidly growing number of public datasets, they often are best used for tutorials, training and establishing baseline metrics. It is extremely difficult to create new, innovative and meaningful data analysis based upon datasets already mined by others. This week we study Web Scraping and APIs so you will be able to create your own unique datasets based upon the wide variety and vast quantity of information available on the Web. Data constantly flows constantly in networks under two basic paradigms . First, Information intended for humans exit computer networks via a interactive and/or visual interfaces. These include (a) a widows-based Graphical User Interfaces (GUIs) like those in MacOS and WindowsOS, and (b) a text-based interactive REPL command line shell (read-execute-print-loop) . Machine-to-Human GUIs are carefully designed to represent, architect, and sequence information to leverage human intuitions and cognitive processes. Unfortunately, programmatically scraping data from web GUIs can be slow, error-prone, fragile and (in many cases) impossible due to the limited resources, continual design updates, technical anti-scraping barriers and legal concerns. Web scraping can be viable for creating unique datasets based upon simpler open web sites or for harvesting smaller high-value data. To web scrape effectively, you will need to learn several core concepts. Content (HTML) and presentation (CSS) metatags organize data within a web page. Specific data within a web page can be uniquely identified and extracted using XPath notation and Beautiful Soup 4. Web frameworks and designs change frequently, are often deeply nested in human-unfriendly fashion and follow no universal pattern which makes web scraping a constant challenge. Secondly, machine-to-machine API communications at the application layer are governed by strictly defined protocols like HTTP/REST and GraphQL passing data in well-structured file formats like JSON and XML. Machine-to-machine communications have the advantages of speed, scalability and programmable automation. Many websites make their data available via open and/or paid access using their own API. Unlike web pages, APIs are data exchange formats are dictated by centralized standards committees, rarely change and share universal best practices which makes exchanging data or creating unique datasets via API fast, efficient and reliable. This week introduces common web scraping tools and programming practices. It is particularly useful to learn this in a collaborative lab environment since there are so many rules and exceptions to the rule it can be hard to quickly get a functional overview. We\u2019ll also practice automating calls to popular REST API services to compile unique datasets. This includes learning the API for Twitter, Reddit and Instagram as well as the libraries to convert between JSON datafiles and internal Python data types like dicts{} and Pandas DataFrames. Applications [Monday]: Web Scraping Basics in Python (19:47) Datacamp.com Web Scraping in Python (2hrs) Chp 1 & 2 [Wednesday]: Beautiful Soup: Build a Web Scraper With Python Datacamp.com Web Scraping in Python (1hrs) Chp 3 [Friday]: Scrape Data From Zappos.com with Chrome Extension: Free Web Scraper Practical XPath for Web Scraping Software to Install VSCode Thunder VS Code Plugin for APIs Virtual Environments with venv New Python Libraries virtualenv requests.py Beautiful Soup 4 lxml In-Class Lab HTML & CSS Crash Course Tutorial #7 - Chrome Dev Tools How to find xpath in Chrome (4:06) Beautiful Soup: Build a Web Scraper With Python Books.toscrape.com XPather Chrome Extension: Google Chrome Free Web Scraper Tutorial Optional Additional References PEP8 Style Guidelines Virtual Environments in Python (13:32)","title":"Week: Scraping"},{"location":"scraping/#week-scraping","text":"","title":"Week: Scraping"},{"location":"scraping/#overview","text":"Data is the Alpha and Omega of Data Science, Machine Learning and Deep Learning. The creativity, expressiveness and strength of your analysis will be bounded by the data you have. The quality of your dataset can be measured along many dimensions including the type/number of features, originality, number of datapoints, accuracy, coherence, etc. While there are a rapidly growing number of public datasets, they often are best used for tutorials, training and establishing baseline metrics. It is extremely difficult to create new, innovative and meaningful data analysis based upon datasets already mined by others. This week we study Web Scraping and APIs so you will be able to create your own unique datasets based upon the wide variety and vast quantity of information available on the Web. Data constantly flows constantly in networks under two basic paradigms . First, Information intended for humans exit computer networks via a interactive and/or visual interfaces. These include (a) a widows-based Graphical User Interfaces (GUIs) like those in MacOS and WindowsOS, and (b) a text-based interactive REPL command line shell (read-execute-print-loop) . Machine-to-Human GUIs are carefully designed to represent, architect, and sequence information to leverage human intuitions and cognitive processes. Unfortunately, programmatically scraping data from web GUIs can be slow, error-prone, fragile and (in many cases) impossible due to the limited resources, continual design updates, technical anti-scraping barriers and legal concerns. Web scraping can be viable for creating unique datasets based upon simpler open web sites or for harvesting smaller high-value data. To web scrape effectively, you will need to learn several core concepts. Content (HTML) and presentation (CSS) metatags organize data within a web page. Specific data within a web page can be uniquely identified and extracted using XPath notation and Beautiful Soup 4. Web frameworks and designs change frequently, are often deeply nested in human-unfriendly fashion and follow no universal pattern which makes web scraping a constant challenge. Secondly, machine-to-machine API communications at the application layer are governed by strictly defined protocols like HTTP/REST and GraphQL passing data in well-structured file formats like JSON and XML. Machine-to-machine communications have the advantages of speed, scalability and programmable automation. Many websites make their data available via open and/or paid access using their own API. Unlike web pages, APIs are data exchange formats are dictated by centralized standards committees, rarely change and share universal best practices which makes exchanging data or creating unique datasets via API fast, efficient and reliable. This week introduces common web scraping tools and programming practices. It is particularly useful to learn this in a collaborative lab environment since there are so many rules and exceptions to the rule it can be hard to quickly get a functional overview. We\u2019ll also practice automating calls to popular REST API services to compile unique datasets. This includes learning the API for Twitter, Reddit and Instagram as well as the libraries to convert between JSON datafiles and internal Python data types like dicts{} and Pandas DataFrames.","title":"Overview"},{"location":"scraping/#applications","text":"[Monday]: Web Scraping Basics in Python (19:47) Datacamp.com Web Scraping in Python (2hrs) Chp 1 & 2 [Wednesday]: Beautiful Soup: Build a Web Scraper With Python Datacamp.com Web Scraping in Python (1hrs) Chp 3 [Friday]: Scrape Data From Zappos.com with Chrome Extension: Free Web Scraper Practical XPath for Web Scraping","title":"Applications"},{"location":"scraping/#software-to-install","text":"VSCode Thunder VS Code Plugin for APIs Virtual Environments with venv","title":"Software to Install"},{"location":"scraping/#new-python-libraries","text":"virtualenv requests.py Beautiful Soup 4 lxml","title":"New Python Libraries"},{"location":"scraping/#in-class-lab","text":"HTML & CSS Crash Course Tutorial #7 - Chrome Dev Tools How to find xpath in Chrome (4:06) Beautiful Soup: Build a Web Scraper With Python Books.toscrape.com XPather Chrome Extension: Google Chrome Free Web Scraper Tutorial","title":"In-Class Lab"},{"location":"scraping/#optional-additional-references","text":"PEP8 Style Guidelines Virtual Environments in Python (13:32)","title":"Optional Additional References"},{"location":"sentiment_analysis/","text":"Week: Sentiment Analysis Overview Cultural analytics is the study of society and social phenomena by analyzing data and the way it flows. This course presumes some coding experience or the introductory course to Digital Humanities, Programming Humanity. We\u2019ll build on our skills using API\u2019s to create original datasets from social media sites like Twitter. Then we\u2019ll develop natural language processing skills including sentiment analysis and topic clustering to explore text for insights. We\u2019ll also learn how to graph and explore social networks. In class, we\u2019ll do some hands-on projects like analyzing the social network of Game of Thrones and trying to classify who\u2019s tweeting: Trump or Trudeau. In the final segment of the course, students develop their own project centered on their interests. Applications [Monday]: Torn Apart Separado, Project Torn Apart Separado, Summary [Wednesday]: Not Even the Past: Social Vunerability and the Legacy of Redlining [Friday]: Land Acquisition and Dispossession: Mapping the Homestead Act, 1863-1912 Coding Practice DataCamp Sentiment Analysis in Python Lab Prep GeoPy Lab Assignment Find at least 2 disparate shape/location datafiles and plot them both onto the same map using GeoPy.","title":"Week: Sentiment Analysis"},{"location":"sentiment_analysis/#week-sentiment-analysis","text":"","title":"Week: Sentiment Analysis"},{"location":"sentiment_analysis/#overview","text":"Cultural analytics is the study of society and social phenomena by analyzing data and the way it flows. This course presumes some coding experience or the introductory course to Digital Humanities, Programming Humanity. We\u2019ll build on our skills using API\u2019s to create original datasets from social media sites like Twitter. Then we\u2019ll develop natural language processing skills including sentiment analysis and topic clustering to explore text for insights. We\u2019ll also learn how to graph and explore social networks. In class, we\u2019ll do some hands-on projects like analyzing the social network of Game of Thrones and trying to classify who\u2019s tweeting: Trump or Trudeau. In the final segment of the course, students develop their own project centered on their interests.","title":"Overview"},{"location":"sentiment_analysis/#applications","text":"[Monday]: Torn Apart Separado, Project Torn Apart Separado, Summary [Wednesday]: Not Even the Past: Social Vunerability and the Legacy of Redlining [Friday]: Land Acquisition and Dispossession: Mapping the Homestead Act, 1863-1912","title":"Applications"},{"location":"sentiment_analysis/#coding-practice","text":"DataCamp Sentiment Analysis in Python","title":"Coding Practice"},{"location":"sentiment_analysis/#lab-prep","text":"GeoPy","title":"Lab Prep"},{"location":"sentiment_analysis/#lab-assignment","text":"Find at least 2 disparate shape/location datafiles and plot them both onto the same map using GeoPy.","title":"Lab Assignment"},{"location":"syllabus/","text":"Syllabus Course Description IPHS 290 Cultural Analytics Integrated Program for Humane Studies Office Hours Professor Elkins TueThur 1-2pm, Fri 1:10-2:10 or appointment at elkinsk@kenyon.edu Professor Chun MWF 1:10-2:10 or appointment at chunj@kenyon.edu Calendar Week Date Topic Motivating Example Technical Background Coding Assignment 1 29 Aug (Mon) Mapping Redlining, Detention Centers, Dispossession & the Homestead Act GeoPy and GeoPandas DataCamp Working w/Geospatial Data in Python 2 5 Sep (Mon) MINI-PROJECT #1 Geospatial Mapping GeoPy and GeoPandas Kaggle GeoSpatial Analysis 3 12 Sep (Mon) Web Scraping Scrape Example HTML, XPaths, CSS, Spiders DataCamp Web Scraping in Python 4 12 Sep (Mon) APIs API and Bots Twitter API, REST, JSON DataCamp Analyzing Social Media in Python 5 26 Sep (Mon) Introduction to NLP RegEx, TF-IDF, NER, Polyglot NLTK, SpaCy, Gensim DataCamp Introduction to NLP in Python 6 7 Nov (Mon) Social Networks GoT and Twitter Examples GraphX and Twitter DataCamp Introduction to Network Analysis in Python 7 x Nov (Mon) MINI-PROJECT #2 Social Networks NetworkX TBA 8 3 Oct (Mon) Sentiment Analysis Parsing and Linguistic Example NLTK & SpaCy DataCamp Sentiment Analysis in Python 9 10 Oct (Mon) Advanced NLP Vectorization, Pipelines, DNN SpaCy DataCamp Advanced NLP with SpaCy 10 17 Oct (Mon) Topic Modeling Topic Modeling Example Gensim Gensim 4 Core Tutorials 11 31 Oct (Mon) Diachronic Sentiment Analysis SentimentArcs Cambridge & Time Series Processing SentimentArcs Paper/Chapter DataCamp Manipulating Time Series Data in Python 12 7 Nov (Mon) MINI-PROJECT #3 Diachronic Sentiment Analysis SentimentArcs TBA 13 14 Nov (Mon) Web Servers Museum Curation GitHub.io and mkdocs Github Pages Mkdocs 14 28 Nov (Mon) Final Project Work in-class on project Digital Kenyon Lab all week Course Description Cultural analytics is the study of society and social phenomena by analyzing data and the way it flows. This involves gathering, transforming, visualizing and narrativizing data in various forms including numeric, textual, geospatial, and as time series. This course presumes some coding experience or the introductory course to Digital Humanities, IPHS200 Programming Humanity . We start with geospatial data and mapping. Next, we\u2019ll build on our skills using web scrapers and API\u2019s to create original datasets from social media sites like Twitter. Then we\u2019ll learn various natural language processing techniques underlying sentiment analysis and topic clustering to explore text for insights. Finally, we'll learn how to graph and explore the relations between data in the form of social networks. This is a methods class focused on learning both the conceptual and practical applications of maps, texts, and networks. A carefully curated sequence of key abstract concepts explored through the most popular and useful libraries will mark our progression through the semester. We'll also read some of the best examples of work in the field and use these to inspire our own experiments. We\u2019ll do some hands-on projects on DataCamp like analyzing the social network of Game of Thrones and exploring the shifting topics/sentiment of Trump/Trudeau on social media. But we'll also have three project weeks during which we'll break into groups and move through the entire process, from brainstorming ideas to visualizing and analyzing the results. In the final segment of the course, we'll learn how to showcase projects on individual Github sites and work on final projects centered on individual interests. By the end of this course you will: Deepen your proficiency in Python, data visualization, and data wrangling Learn how to automate dataset creation via web scraping and programmic APIs Learn the fundamentals of geospatial analytics Learn core Natural Language Processing techniques including scraping, cleaning, representing, transforming and analyzing text Learn the fundamentals time series analysis Learn basic graph theory, algorithms, metrics and visualizations Present your code on Github and project on a personal website Learn how to identify interesting multi-disciplinary research questions and judiciously use a wide range of computational technologies to analyze, critique and productively address such questions Grading 25% DataCamp 15% Reading/Concept Quizzes 40% 3 Mini-projects 20% Final Project There is no final exam Attendance In accordance with standard Kenyon policy, absences greater than 25% of the class will result in a failing grade. You are allowed three absences, no questions asked, before your attendance grade drops. These absences count as combination sick/personal days, i.e. please do not take 3 personal days and then ask for additional sick days. In cases of extreme illness or other unforeseen events, please ensure the advising office is aware and that you\u2019ve been granted an excused absence. We will be notified accordingly. Quizzes Short quizzes will test comprehension of key terms and information. Quizzes cannot be made up if you miss class. 3 quizzes will be dropped to accomodate 3 absences. Then the 3 lowest grades will be dropped. Datacamp Assignments and Mini-Projects You will receive a free Datacamp account with assignments due for each class session. Assignments are graded solely on completion, but you must complete them fully (i.e. no 0\u2019s). Please make sure the checkmarks are there on each section on the outline of the course. Final Project For the final project, you may a) complete a data analysis or cultural analytics project working with numeric or linguistic data or b) create an in-depth analysis of a particular technology. b) should demonstrate understanding of both the technology and the ethical/social issues surrounding that technology. Final Thoughts on Grades This class is meant to be a fun and exploratory introduction. Students will bring different strengths and backgrounds to this interdisciplinary class, and the emphasis will be on developing these personal interests and competencies. If you do the work, you will do well. Responsible Employee Information We will be studying and/or discussing a number of issues that may cause discomfort or distress. If you wish to speak with either of us about any readings, assignments or class discussions, please understand that we may be required to report information about sexual misconduct to the Title IX Coordinator. For confidential support, you may contact the following resources: The Health and Counseling Center, Sexual Misconduct Advisors (SMAs) the College chaplains, and staff at New Directions Domestic Abuse Shelter & Rape Crisis Center Statement of Academic Integrity and Disability Accommodations At Kenyon we expect all students, at all times, to submit work that represents the highest standards of academic integrity. It is the responsibility of each student to learn and practice the proper ways of documenting and acknowledging those whose ideas and words they have drawn upon (see Academic Honesty and Questions of Plagiarism in the Course Catalog). Ignorance and carelessness are not excuses for academic dishonesty. If you are uncertain about the expectations for this class, please ask for clarification. Students with disabilities who will be taking this course and may need academic accommodations are encouraged to make an appointment to see me as soon as possible. Also, you are required to register for support services with the Office of Disability Services in the Olin Library, Center for Innovative Pedagogy.","title":"Syllabus"},{"location":"syllabus/#syllabus","text":"","title":"Syllabus"},{"location":"syllabus/#course-description","text":"IPHS 290 Cultural Analytics Integrated Program for Humane Studies","title":"Course Description"},{"location":"syllabus/#office-hours","text":"Professor Elkins TueThur 1-2pm, Fri 1:10-2:10 or appointment at elkinsk@kenyon.edu Professor Chun MWF 1:10-2:10 or appointment at chunj@kenyon.edu","title":"Office Hours"},{"location":"syllabus/#calendar","text":"Week Date Topic Motivating Example Technical Background Coding Assignment 1 29 Aug (Mon) Mapping Redlining, Detention Centers, Dispossession & the Homestead Act GeoPy and GeoPandas DataCamp Working w/Geospatial Data in Python 2 5 Sep (Mon) MINI-PROJECT #1 Geospatial Mapping GeoPy and GeoPandas Kaggle GeoSpatial Analysis 3 12 Sep (Mon) Web Scraping Scrape Example HTML, XPaths, CSS, Spiders DataCamp Web Scraping in Python 4 12 Sep (Mon) APIs API and Bots Twitter API, REST, JSON DataCamp Analyzing Social Media in Python 5 26 Sep (Mon) Introduction to NLP RegEx, TF-IDF, NER, Polyglot NLTK, SpaCy, Gensim DataCamp Introduction to NLP in Python 6 7 Nov (Mon) Social Networks GoT and Twitter Examples GraphX and Twitter DataCamp Introduction to Network Analysis in Python 7 x Nov (Mon) MINI-PROJECT #2 Social Networks NetworkX TBA 8 3 Oct (Mon) Sentiment Analysis Parsing and Linguistic Example NLTK & SpaCy DataCamp Sentiment Analysis in Python 9 10 Oct (Mon) Advanced NLP Vectorization, Pipelines, DNN SpaCy DataCamp Advanced NLP with SpaCy 10 17 Oct (Mon) Topic Modeling Topic Modeling Example Gensim Gensim 4 Core Tutorials 11 31 Oct (Mon) Diachronic Sentiment Analysis SentimentArcs Cambridge & Time Series Processing SentimentArcs Paper/Chapter DataCamp Manipulating Time Series Data in Python 12 7 Nov (Mon) MINI-PROJECT #3 Diachronic Sentiment Analysis SentimentArcs TBA 13 14 Nov (Mon) Web Servers Museum Curation GitHub.io and mkdocs Github Pages Mkdocs 14 28 Nov (Mon) Final Project Work in-class on project Digital Kenyon Lab all week","title":"Calendar"},{"location":"syllabus/#course-description_1","text":"Cultural analytics is the study of society and social phenomena by analyzing data and the way it flows. This involves gathering, transforming, visualizing and narrativizing data in various forms including numeric, textual, geospatial, and as time series. This course presumes some coding experience or the introductory course to Digital Humanities, IPHS200 Programming Humanity . We start with geospatial data and mapping. Next, we\u2019ll build on our skills using web scrapers and API\u2019s to create original datasets from social media sites like Twitter. Then we\u2019ll learn various natural language processing techniques underlying sentiment analysis and topic clustering to explore text for insights. Finally, we'll learn how to graph and explore the relations between data in the form of social networks. This is a methods class focused on learning both the conceptual and practical applications of maps, texts, and networks. A carefully curated sequence of key abstract concepts explored through the most popular and useful libraries will mark our progression through the semester. We'll also read some of the best examples of work in the field and use these to inspire our own experiments. We\u2019ll do some hands-on projects on DataCamp like analyzing the social network of Game of Thrones and exploring the shifting topics/sentiment of Trump/Trudeau on social media. But we'll also have three project weeks during which we'll break into groups and move through the entire process, from brainstorming ideas to visualizing and analyzing the results. In the final segment of the course, we'll learn how to showcase projects on individual Github sites and work on final projects centered on individual interests.","title":"Course Description"},{"location":"syllabus/#by-the-end-of-this-course-you-will","text":"Deepen your proficiency in Python, data visualization, and data wrangling Learn how to automate dataset creation via web scraping and programmic APIs Learn the fundamentals of geospatial analytics Learn core Natural Language Processing techniques including scraping, cleaning, representing, transforming and analyzing text Learn the fundamentals time series analysis Learn basic graph theory, algorithms, metrics and visualizations Present your code on Github and project on a personal website Learn how to identify interesting multi-disciplinary research questions and judiciously use a wide range of computational technologies to analyze, critique and productively address such questions","title":"By the end of this course you will:"},{"location":"syllabus/#grading","text":"25% DataCamp 15% Reading/Concept Quizzes 40% 3 Mini-projects 20% Final Project There is no final exam","title":"Grading"},{"location":"syllabus/#attendance","text":"In accordance with standard Kenyon policy, absences greater than 25% of the class will result in a failing grade. You are allowed three absences, no questions asked, before your attendance grade drops. These absences count as combination sick/personal days, i.e. please do not take 3 personal days and then ask for additional sick days. In cases of extreme illness or other unforeseen events, please ensure the advising office is aware and that you\u2019ve been granted an excused absence. We will be notified accordingly.","title":"Attendance"},{"location":"syllabus/#quizzes","text":"Short quizzes will test comprehension of key terms and information. Quizzes cannot be made up if you miss class. 3 quizzes will be dropped to accomodate 3 absences. Then the 3 lowest grades will be dropped.","title":"Quizzes"},{"location":"syllabus/#datacamp-assignments-and-mini-projects","text":"You will receive a free Datacamp account with assignments due for each class session. Assignments are graded solely on completion, but you must complete them fully (i.e. no 0\u2019s). Please make sure the checkmarks are there on each section on the outline of the course.","title":"Datacamp Assignments and Mini-Projects"},{"location":"syllabus/#final-project","text":"For the final project, you may a) complete a data analysis or cultural analytics project working with numeric or linguistic data or b) create an in-depth analysis of a particular technology. b) should demonstrate understanding of both the technology and the ethical/social issues surrounding that technology.","title":"Final Project"},{"location":"syllabus/#final-thoughts-on-grades","text":"This class is meant to be a fun and exploratory introduction. Students will bring different strengths and backgrounds to this interdisciplinary class, and the emphasis will be on developing these personal interests and competencies. If you do the work, you will do well.","title":"Final Thoughts on Grades"},{"location":"syllabus/#responsible-employee-information","text":"We will be studying and/or discussing a number of issues that may cause discomfort or distress. If you wish to speak with either of us about any readings, assignments or class discussions, please understand that we may be required to report information about sexual misconduct to the Title IX Coordinator. For confidential support, you may contact the following resources: The Health and Counseling Center, Sexual Misconduct Advisors (SMAs) the College chaplains, and staff at New Directions Domestic Abuse Shelter & Rape Crisis Center","title":"Responsible Employee Information"},{"location":"syllabus/#statement-of-academic-integrity-and-disability-accommodations","text":"At Kenyon we expect all students, at all times, to submit work that represents the highest standards of academic integrity. It is the responsibility of each student to learn and practice the proper ways of documenting and acknowledging those whose ideas and words they have drawn upon (see Academic Honesty and Questions of Plagiarism in the Course Catalog). Ignorance and carelessness are not excuses for academic dishonesty. If you are uncertain about the expectations for this class, please ask for clarification. Students with disabilities who will be taking this course and may need academic accommodations are encouraged to make an appointment to see me as soon as possible. Also, you are required to register for support services with the Office of Disability Services in the Olin Library, Center for Innovative Pedagogy.","title":"Statement of Academic Integrity and Disability Accommodations"},{"location":"topic_modeling/","text":"Week: Topic Modeling Overview Cultural analytics is the study of society and social phenomena by analyzing data and the way it flows. This course presumes some coding experience or the introductory course to Digital Humanities, Programming Humanity. We\u2019ll build on our skills using API\u2019s to create original datasets from social media sites like Twitter. Then we\u2019ll develop natural language processing skills including sentiment analysis and topic clustering to explore text for insights. We\u2019ll also learn how to graph and explore social networks. In class, we\u2019ll do some hands-on projects like analyzing the social network of Game of Thrones and trying to classify who\u2019s tweeting: Trump or Trudeau. In the final segment of the course, students develop their own project centered on their interests. Applications [Monday]: Mining the Dispatch [Wednesday]: The Largest Vocabulary in Hip Hop [Friday]: Fan Engagement Meter Coding Practice Gensim 4 Core Tutorials Lab Prep GeoPy Lab Assignment Find at least 2 disparate shape/location datafiles and plot them both onto the same map using GeoPy.","title":"Week: Topic Modeling"},{"location":"topic_modeling/#week-topic-modeling","text":"","title":"Week: Topic Modeling"},{"location":"topic_modeling/#overview","text":"Cultural analytics is the study of society and social phenomena by analyzing data and the way it flows. This course presumes some coding experience or the introductory course to Digital Humanities, Programming Humanity. We\u2019ll build on our skills using API\u2019s to create original datasets from social media sites like Twitter. Then we\u2019ll develop natural language processing skills including sentiment analysis and topic clustering to explore text for insights. We\u2019ll also learn how to graph and explore social networks. In class, we\u2019ll do some hands-on projects like analyzing the social network of Game of Thrones and trying to classify who\u2019s tweeting: Trump or Trudeau. In the final segment of the course, students develop their own project centered on their interests.","title":"Overview"},{"location":"topic_modeling/#applications","text":"[Monday]: Mining the Dispatch [Wednesday]: The Largest Vocabulary in Hip Hop [Friday]: Fan Engagement Meter","title":"Applications"},{"location":"topic_modeling/#coding-practice","text":"Gensim 4 Core Tutorials","title":"Coding Practice"},{"location":"topic_modeling/#lab-prep","text":"GeoPy","title":"Lab Prep"},{"location":"topic_modeling/#lab-assignment","text":"Find at least 2 disparate shape/location datafiles and plot them both onto the same map using GeoPy.","title":"Lab Assignment"},{"location":"web/","text":"Week: Web Servers Overview Cultural analytics is the study of society and social phenomena by analyzing data and the way it flows. This course presumes some coding experience or the introductory course to Digital Humanities, Programming Humanity. We\u2019ll build on our skills using API\u2019s to create original datasets from social media sites like Twitter. Then we\u2019ll develop natural language processing skills including sentiment analysis and topic clustering to explore text for insights. We\u2019ll also learn how to graph and explore social networks. In class, we\u2019ll do some hands-on projects like analyzing the social network of Game of Thrones and trying to classify who\u2019s tweeting: Trump or Trudeau. In the final segment of the course, students develop their own project centered on their interests. Applications [Monday]: Introduction to Github [Wednesday]: Github Pages [Friday]: Mkdocs and Material Design Coding Practice Github Pages Mkdocs Lab Prep GeoPy Lab Assignment Find at least 2 disparate shape/location datafiles and plot them both onto the same map using GeoPy.","title":"Week: Web Servers"},{"location":"web/#week-web-servers","text":"","title":"Week: Web Servers"},{"location":"web/#overview","text":"Cultural analytics is the study of society and social phenomena by analyzing data and the way it flows. This course presumes some coding experience or the introductory course to Digital Humanities, Programming Humanity. We\u2019ll build on our skills using API\u2019s to create original datasets from social media sites like Twitter. Then we\u2019ll develop natural language processing skills including sentiment analysis and topic clustering to explore text for insights. We\u2019ll also learn how to graph and explore social networks. In class, we\u2019ll do some hands-on projects like analyzing the social network of Game of Thrones and trying to classify who\u2019s tweeting: Trump or Trudeau. In the final segment of the course, students develop their own project centered on their interests.","title":"Overview"},{"location":"web/#applications","text":"[Monday]: Introduction to Github [Wednesday]: Github Pages [Friday]: Mkdocs and Material Design","title":"Applications"},{"location":"web/#coding-practice","text":"Github Pages Mkdocs","title":"Coding Practice"},{"location":"web/#lab-prep","text":"GeoPy","title":"Lab Prep"},{"location":"web/#lab-assignment","text":"Find at least 2 disparate shape/location datafiles and plot them both onto the same map using GeoPy.","title":"Lab Assignment"}]}